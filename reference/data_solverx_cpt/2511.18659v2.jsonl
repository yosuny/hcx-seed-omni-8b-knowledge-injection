{"text": "CLaRa: Bridging Retrieval and Generation\nwith Continuous Latent Reasoning\nJieHe1,2,RichardHeBai1,SineadWilliamson1,JeffZ.Pan2,NavdeepJaitly1‚Ä†,YizheZhang1\n1Apple 2UniversityofEdinburgh\nRetrieval-augmentedgeneration(RAG)enhanceslargelanguagemodels(LLMs)withexternalknowledgebutstillsuffers\nfromlongcontextsanddisjointretrieval‚Äìgenerationoptimization. Inthiswork,weproposeCLaRa(ContinuousLatent\nReasoning),aunifiedframeworkthatperformsembedding-basedcompressionandjointoptimizationinasharedcontinuous\nspace.Toobtainsemanticallyrichandretrievablecompressedvectors,weintroduceSCP,akey-preservingdatasynthesis\nframeworkusingQAandparaphrasesupervision.CLaRathentrainsthererankerandgeneratorend-to-endviaasingle\nlanguagemodelingloss,withgradientsflowingthroughbothmodulesusingadifferentiabletop-kestimator.Theoretically,\nthisunifiedoptimizationalignsretrievalrelevancewithanswerquality.ExperimentsacrossmultipleQAbenchmarksshow\nthatCLaRaachievesstate-of-the-artcompressionandrerankingperformance,oftensurpassingtext-basedfine-tuned\nbaselines.\nCode: https://github.com/apple/ml-clara\nCorrespondence: JieHe:j.he@ed.ac.uk;YizheZhang:yizhe_zhang@apple.com\nDate: November27,2025\n1 Introduction\nRetrieval-Augmented Generation (RAG)hasbecomeapowerfulparadigmforenhancinglargelanguagemodels\n(LLMs) across diverse NLP tasks (Lewis et al., 2020; Gao et al., 2024; Li et al., 2024b; Wu et al., 2024;\nAbootorabi et al., 2025). By incorporating external evidence, RAG mitigates key weaknesses of LLMs such as\nhallucination (Ayala & Bechard, 2024) and knowledge obsolescence (Lau et al., 2025).\nMost RAG systems suffer from a fundamental structural issue: retrieval and generation are optimized\nseparately. Retrievers select documents based on surface-level similarity, while generators produce answers\nwithout providing feedback about what information is truly needed (Shi et al., 2025). This disjoint design\nleads to two intertwined challenges. (1) Optimization. Because document selection is discrete, gradients\ncannot flow from the generator back to the retriever (Sachan et al., 2021; Lin et al., 2024), hindering joint\ntraining and preventing the retriever from aligning with the generator‚Äôs task objective. (2) Efficiency. Dense\nretrievers rank documents in embedding space, whereas generators still consume raw text, resulting in an\narchitectural mismatch. This mismatch yields (i) inconsistent representation spaces that prevent end-to-end\noptimization, (ii) redundant text processing that increases inference cost (Merth et al., 2024) and causes\ncontext overflow (Leng et al., 2024; Yue et al., 2025), and (iii) duplicated encoding for both retrieval and\ngeneration. Even if gradients could flow jointly, these inefficiencies would persist due to the lack of a shared\nlatent space.\nTowardstacklingthisissue,weproposeaunifiedframework\nOurKeyInsight:SharedContinuousRepresentations.\nthat performs retrieval and generation over shared continuous document representations as shown in\nFig 1. Instead of maintaining separate embeddings and raw text, we encode documents once into compact\n‚Ä†WorkdoneatApple.\n1\n5202\nvoN\n52\n]LC.sc[\n2v95681.1152:viXra", "meta": {"page": 1, "source": "2511.18659v2.pdf"}}
{"text": "Compressor Pre-Training Stage Query: Query embedding Doc embeddings Answer:\nRaw Docs Compressed Docs Salient info Which Retrieve New\ncity is the York City\nCompressor Generator living\nplace of Big Stone Gapis a 2014\nthe city, big, Stone, A co m m e e r d ic y a f n i l d m r a d m ir a e c r t o e m d a n b t y i c\ndirector ciudad, Based, Adriana Trigiani.The story is\nre Q a u s e o r n y e r Retrieved Generator o ro f m th an e t ic r V lo o i v r m , g a K in n a i t t a i e c , , , c s R h t a o o r m , m , v i a n r g e , , s o e f t B i i n g t S h t e o n ac e t G ua a l p V c i i r r g c i a n i 1 a 9 7 t 0 ow s. n\nCompressed Docs comedy d an ir a e , c l t o o c r a , t f io il n m , , v i r l i l , a ge, Adriana Trigiani is an Italian\nQuery Co t n o t k i e n n u s ous Backward Forward Answer ‚Äò G B a ig p ‚Äô S ? tone r m e o s v i i d e ence, lives, A t d V e i i m r l l l e e e a c v g r i t e i s c o , i a o r N n n b e b w a w e s r e s i Y t t d e o - i r s r n , e k a l G l n i C r n d i e g t e y f a n i . u l w m t i h c o h r ,\nCLaRa End-to-end Training Stage CLaRa End-to-end Inference Stage\nFigure1 (a) During training, we first pretrain the compressor to encourage it to retain only essential information.\nNext,weperformofflinecompressionofthedocuments. Afterthat,weencodethequeryusingthequeryreasoner,\nretrieve the compressed document representations for generation, and use only the final next-token prediction\nloss to jointly up Q d ue a ry t e both the query reasoner and the generator. (b) An example from the inference stage: the\ntokens representreaksoenyerclue wo E r m dW be so d r d dr i n e g l ated to the question. When we decode the continuous query embedding, we find\nthat it contains infùêêorm L a oo t k i u o p n noMtatprixresent in the original query, indicating that it has learned some of the intermediate\nreasoning keywords.\nRelated documents: Adriana Trigiani is an Italian American best-selling author, television\nwriter, and film director based in Greenwich Village, New York City.\nBig Stone Gap is a 2014 American drama romantic comedy\nmemory-tokfeilmn wrriettpenr aensd edinretcatetdi boyn Asdritanha aTrtigisanei.r Tvhee stboroy tish setp inu trhpe aocstueasl V.irgAinia ctoewnn tofr al motivation behind this design is that\nBig Stone Gap circa 1970s.\nsupervisedretrievaltrainingtypicallyreliesonrelevance-labeleddata,whichisscarceandoftendomain-specific.\nTo overcome this limitation, we propagate the next-token prediction (NTP) loss from the generator to the\nretriever, providing a weakly supervised signal that naturally adapts retrieval to downstream generation\nobjectives. This mechanism allows the retriever to learn which documents truly enhance answer generation\nrather than relying on surface-level similarity. Moreover, continuous representations and joint optimization\nare inherently complementary: continuous encodings make the retrieval process differentiable, while joint\ntraining aligns both modules within a shared semantic space optimized for reasoning.\nThis unified design resolves both challenges simultaneously. Optimization-wise, continuous representations\nenable differentiable top-k selection via Straight-Through estimation, allowing generator gradients to update\nthe retriever directly through gradient descent rather than inefficient RL sampling. Efficiency-wise, shared\nencodings remove redundant computation and drastically reduce context length, enabling fully end-to-end\noptimization and inference within the same representation space.\nTo realize this vision, we present CLaRa (Continuous Latent Reasoning), a joint retrieval‚Äìgeneration\nframework built on shared compressed representations. In Stage I, we propose SCP (Salient Compressor\nPretraining), which enhances semantic fidelity by constructing QA pairs that emphasize salient document\ncontent beyond surface reconstruction. In Stage II, CLaRa performs end-to-end joint training of the\nquery encoder and answer generator under a unified next-token prediction loss, with differentiable top-k\nselectionviaStraight-Throughestimation. Theoretically, weshowthisunifiedobjectiveyieldsvalidgradients\nfor retriever learning without explicit labels.\nWe evaluate CLaRa on four single-hop and multi-hop QA benchmarks with Mistral-7B and Phi-4B. Results\nshow that SCP produces semantically rich compressed representations, and CLaRa achieves state-of-the-art\nretrieval and generation performance‚Äîoutperforming both supervised and unsupervised baselines, and even\nsurpassing full-text fine-tuned models on several tasks.\n2 SCP:SalientCompressorPretraining\nPrevious methods (Louis et al., 2025a; Cheng et al., 2025) typically use token-level reconstruction loss to learn\ndocrepresentation. However, thelearnedrepresentationmaywastelimitedcapacity/budgetontoken-by-token\nreconstruction which might be trivial. Also, the raw representation learned in such a way might not ‚Äúdigest‚Äù\nthedocumentexhaustively. Toenablethemodeltofocusonabstractinganddigestingsemanticallyinformative\nrepresentations, we first synthesize pre-training data that highlights salient information. Based on this data,\nwe train a compression framework, where a compressor learns to retain merely essential semantics (Figure 2).\n2", "meta": {"page": 2, "source": "2511.18659v2.pdf"}}
{"text": "Figure2 OverviewoftheSCP(Salient Compressor Pretraining)framework. Itincludes(a)syntheticdataconstruction\nfor pretraining, (2) compressor training using the pretraining data.\n2.1 GuidedDataSynthesisforSemanticPreservation\nWe first construct a synthetic dataset where salient information is explicitly exposed through QA and\nparaphrasing. This way, the compressor later learns to identify, digest and retain the semantic core of the\ntext by deeply processing the raw token-level information. As shown in Figure 2 (a), our synthetic data\ngenerationpipelineconsistsseveralsteps: (1)salientinformationelicitationviaQAandparaphrasegeneration,\n(2) automatic verification of coverage, and (3) regeneration of missing content.\nUsing 2M sampled Wikipedia-2021 documents (Izacard et al., 2023), a locally\nSalientInformationElicitation.\ndeployed LLM (Qwen-32B) generates three complementary forms of supervision:\n‚Ä¢ Simple QA: each pair captures a single fact, encouraging fine-grained factual retention. To avoid\nredundancy, the model is guided to extract distinct atomic facts that have not been covered by previous\nquestions. This helps ensure broad coverage of salient information while keeping each question focused.\nExample: Q: ‚ÄúIn which plant family is Trichocladus crinitus classified?‚Äù A: ‚ÄúTrichocladus crinitus\nbelongs to the plant family Hamamelidaceae (the witch-hazel family).‚Äù\n‚Ä¢ Complex QA: each pair integrates multiple facts, promoting relational reasoning and higher-level\nabstraction. During generation, the model is encouraged to connect previously unlinked facts, which\nreduces repetition and increases coverage of relational information. Example: Q: ‚ÄúWhich player joined\nNewport County on 2 October 2014 and made his debut as a second-half substitute in a 1‚Äì0 defeat at\nOxford United on 4 October?‚Äù A: ‚ÄúThe player is James Loveridge.‚Äù\n‚Ä¢ Paraphrase: paraphrased documents reorder sentences structure, thus altering the surface form of the\ntextwhilepreservingitscoresemantics. Learningsuchamappingfromoriginaltexttoparaphrasedtext\nthrough an information bottleneck of continuous representation will enable the learned representation\nto focus on the semantics.\nQA pairs distill fact-centric supervision as they tell the model which details are essential for answering\nmeaningful questions. Paraphrases, in contrast, demonstrate expression, level compactness, how to rephrase\nthesamecontentmoreefficiently. Together,theyformcomplementarysignals: factualgroundingandlinguistic\ncompactness.\nEach document and its generated outputs (QA pairs or paraphrases) are verified\nVerificationandRegeneration.\nby the same LLM for factual consistency and information coverage. When missing information is detected,\n3", "meta": {"page": 3, "source": "2511.18659v2.pdf"}}
{"text": "the LLM reviews both the original text and existing QA pairs to generate additional ones capturing uncovered\nfacts, iteratively up to ten rounds. Samples failing final coverage criteria are excluded. This iterative check\nensures the model only learns from fully covered, factually faithful pairs.\n2.2 CompressorPretraining\nFollowing PISCO (Louis et al., 2025a), we adopt a shared base model equipped with multiple LoRA adapters\nfor modular control, where each adapter corresponds to a distinct function (compression or generation) as\nillustrated in Figure 2 (b).\nCompressionandGeneration. Given a document d i = {t 1 ,...,t m }, we append l learnable memory tokens\n(m ,...,m ) and activate only the compressor LoRA Œ∏ . The final-layer hidden states of the memory tokens\n1 l c\nform the compressed representation:\nM =LLM ([t ,...,t ,m ,...,m ])[m+1:m+l].\ni Œ∏c 1 m 1 l\nThe compressed vector M is concatenated with an instruction I to form T =[I;M ]. During pretraining, I\ni i\ncorrespondstogeneraltext-generationtasks(e.g.,QAorparaphrasing),andlaterisreplacedwithtask-specific\nprompts during instruction tuning. Only the generator LoRA Œ∏ is trained via cross-entropy loss:\ng\n|R‚àó|\nL (Œ∏ ,Œ∏ )=‚àí (cid:88) (cid:88)i logp (cid:0) a‚àó |I,M ,a‚àó (cid:1) . (2.1)\nCE c g Œ∏g i,t i i,<t\n(di,I,R\ni\n‚àó)t=1\nTo ensure that the compressed representation faithfully reflects the semantics of the\nCompressionAlignment.\noriginaldocument,weencouragetheirlatentrepresentationstoremainaligned. Intuitively,thememorytokens\nshould summarize the same semantic space as the document tokens, rather than drifting to unrelated regions.\nTherefore, we minimize the mean squared error (MSE) between the averaged hidden states of document\ntokens and memory tokens:\n(cid:13) (cid:13)2\n(cid:13) l (cid:13)\nL MSE = (cid:13) (cid:13) (cid:13)|d 1 | (cid:88) h t ‚àí 1 l (cid:88) h mj (cid:13) (cid:13) (cid:13) . (2.2)\n(cid:13) i t‚ààdi j=1 (cid:13)\n2\nThe total training loss is:\nL =L +ŒªL , (2.3)\ntotal CE MSE\nwhere Œª balances semantic alignment and generative quality.\nThepretrainedcompressorisforgeneralpurpose. Toadaptthecompressorfordownstream\nInstructionTuning.\nQA and also obtain an answer generator that can comprehend the continuous document representation, we\noptionally performed an additional instruction finetuning training. Note that this step is not necessary. It is\nfor evaluation and ablation purpose, and we show the comparison in our experiments.\nTo achieve this, we use downstream training datasets in which the retrieved documents are paired with task\ninstructions. These document‚Äìinstruction pairs form the input to the model, while the output is a reference\nresponse generated by a teacher model conditioned on the same retrieved documents and instructions. Similar\nto the compressor pretraining stage, we jointly finetune the LoRA adapters of both the compressor and the\nanswer generator during this instruction-tuning process.\n3 CLaRa:Retrievalandgenerationjointtraining\nWhile the compressor distills documents into compact representations, a key challenge is how to retrieve and\nleverage these representations effectively for downstream reasoning. Conventional RAG systems train the\nretriever and generator separately, often resulting in a mismatch between what retrieval considers ‚Äúrelevant‚Äù\nand what generation truly needs, as well as two isolated representation spaces. To address this, we propose\n4", "meta": {"page": 4, "source": "2511.18659v2.pdf"}}
{"text": "Figure3 CLaRa end-to-end training: update query reasoner (Œ∏ qr ) and generator (Œ∏ g ) via language modeling loss using\ncandidate document‚Äìquestion‚Äìanswer triples.\nCLaRa, which unifies retrieval and generation by training both within a single pretrained LLM through a\ndifferentiable retrieval module. Achieving end-to-end training, however, requires a retrieval space that is\nboth stable and computationally manageable‚Äîfull documents are far too large to be re-indexed throughout\noptimization. To solve this, we use the compressor trained in SCP to produce high-quality compact\nrepresentations that remain stable even when the rest of the model updates. By retrieving over these frozen\ncompressed vectors, CLaRa can support end-to-end optimization of retrieval and generation using only a\nshared cross-entropy loss, without requiring relevance-labeled data.\nAs shown in Figure 3, each of the document is compressed into a dense embedding\nFrameworkOverview\nM = Œ∏ (t ) using a pretrained compressor Œ∏ . The compressor remains frozen to allow offline document\ni c i c\nencoding. Wethentrainaqueryreasoner(Œ∏ ), aLoRAadapterinitializedfromŒ∏ , torepresentqueriesinthe\nqr c\nsame space and with the same number of memory tokens as document representations. Through next-token\nprediction (NTP) training, Œ∏ learns not only to encode query intent but also to anticipate relevant document\nqr\ncontent, enhancing retrieval and answer generation. For example, given ‚ÄúWhich city hosted the first modern\nOlympic Games?‚Äù, an embedding-based retriever may miss ‚Äúfirst‚Äù or ‚Äúmodern,‚Äù whereas the NTP-trained\nquery reasoner favors documents mentioning ‚ÄúAthens 1896,‚Äù which better satisfies retrieval with reasoning\nneeds. We use cosine similarity to determine the relevance between query and documents:\ns =cos(q,M ), i=1,...,D. (3.1)\ni i\nThetop-k documentcompressedembeddings{M ,...,M }areconcatenatedwithQandfedtothegenerator\n1 k\n(activated by Œ∏ ), which produces the final answer. During training, both Œ∏ and Œ∏ are updated via the\ng qr g\nunified language modeling loss:\n|R‚àó|\nL\nCLaRa\n(Œ∏\nq\n,Œ∏\ng\n)=‚àí (cid:88) logp\nŒ∏g\n(cid:0) a‚àó\nt\n(cid:12) (cid:12)Q,M\n(1:k)\n,a‚àó\n<t\n(cid:1) , (3.2)\nt=1\nwhere R‚àó = (a‚àó,...,a‚àó ) denotes the reference output. Importantly, this allows the retriever (implicitly\n1 |R‚àó|\nrepresentedbyŒ∏ )tolearn through weak supervision from the generation objective, withoutexplicit\nqr\nreranking labels. Finding real supervised data might be challenging, and our method is data free as it relies\nonly on downstream next token prediction objective to reason on how to retrieve the doc that maximize the\nlikelihood of downstream generation, thus is more flexible and adaptive.\nDifferentiableTop-kSelection In the CLaRa framework, retrieval and generation are trained jointly, yet\ntheir connection is mediated by the top-k selection of relevant documents. However, this discrete operation\nintroduces a broken gradient problem: the generator‚Äôs supervision cannot propagate back to inform the\nretriever why certain documents should be preferred over others. To address this issue, we introduce top\nK selector via Straight-Through (ST) estimator, which conceptually acts as a ‚Äúsoft lens‚Äù ‚Äî preserving the\ndiscrete retrieval behavior during inference while allowing smooth gradient feedback during training (see\nAlgorithm 1 in Appendix for details).\n5", "meta": {"page": 5, "source": "2511.18659v2.pdf"}}
{"text": "Given cosine similarities s=[s ,...,s ], temperature œÑ, and masking for previously selected items, the soft\n1 D\nand hard selections are defined as:\n(cid:18) (cid:19)\ns +log(mask +Œµ)\nZ [b,j,:]=softmax b b , (3.3)\nsoft œÑ\n(cid:40)\n1, if i=argmax Z [b,j,i‚Ä≤],\nZ [b,j,i]= i‚Ä≤ soft (3.4)\nhard 0, otherwise,\nand the final objective that combines the hard and soft representations through a ST estimator is:\nZ =Z + (cid:0) Z ‚àíSG(Z ) (cid:1) , (3.5)\nhard soft soft\nwhere SG(¬∑) denotes the stop-gradient operator. This maintains discrete behavior in the forward pass while\nenablingdifferentiabletrainingthroughZ . Theaggregatedtop-kdocumentrepresentationisthencomputed\nsoft\nas:\nM(k) =ZM, (3.6)\nwhere M ‚ààRB√óD√ód is the matrix of all candidate embeddings with B denoting the batch size, D the number\nof candidate documents, and d the dimensionality of each document representation, typically defined as the\nproduct of the number of memory tokens and the hidden dimension of the underlying LLM.\nWe provide a theoretical justification for why learning from\nTheoreticalJustification:GradientCouplingAnalysis\nNTP may yield stronger and more stable learning signals for the Œ∏ . Note that Œ∏ can also be pre-trained\nqr qr\nindependently using contrastive learning over positive and negative document‚Äìquery pairs (d+,q) and (d‚àí,q),\nfollowing the DPR framework (Zhou & Chen, 2025). For query x and document d, the retrieval score s\nxd\ndefines\np(d|x)= exp(s xd /œÑ) , p(y|x)= (cid:88) p(d|x)p(y|x,d), L=‚àílogp(y|x).\n(cid:80)\nexp(s /œÑ)\nj xj d\nWhen reranking and generation share same representations, p(y|x,d) depends on s , allowing generator\nxd\ngradientstoflowintothereranker. Withsharedembeddingsr = (cid:80) œÄ (s)z andaStraight-Throughestimator\nd d d\nfor top-k, the gradient is\n(cid:20) (cid:21)\n‚àÇL =‚àí 1 p(d|x) (cid:0) p(y|x,d)‚àíp(y|x) (cid:1) + p(y|x,r) p(d|x)g‚ä§(z ‚àír) ,\n‚àÇs p(y|x) œÑ d\nxd\nwhere g =‚àá logp(y|x,r).\nr\nInterpretation: Intuitively, the gradient coupling allows the retriever to receive two complementary\nlearning signals. First, it is rewarded for ranking the correct documents higher through probabilistic\nalignmentbetweenp(d|x)andp(y|x,d). Second,itisguidedtorepresentdocumentsinawaythatfacilitates\nthe generator‚Äôs reasoning via representation-level feedback from the gradient g. Together, these dual signals\nstabilize training‚Äîrather than oscillating between strong retrieval and weak generation, both modules\nprogressively align within a shared semantic space, ensuring consistent retrieval‚Äìgeneration behavior (see\nAppendix A for details).\nCaseStudy:QueryReasonerŒ∏\nqr\nTo probe the information embedded within the query reasoner Œ∏\nqr\n, we adopt\nthelogit lens analysistechnique(nostalgebraist,2020). Foreachmemoryembedding,weprojectitthroughthe\nLLM‚Äôs output head and record the top-50 tokens with the highest logits as topic tokens. We then aggregate\nand filter these decoded tokens to remove trivial elements such as punctuation or special symbols. As shown\nin Table 1, for the query ‚ÄúHow many yards did the nephew of Ivory Lee Brown get during his 2004 true\nfreshman season?‚Äù, the query embeddings decoded from the reasoner include the token ‚ÄúNFL‚Äù, ‚ÄúOklahoma‚Äù,\ndespite the fact that this word does not appear in the question itself. Interestingly, this token also occurs in\nthe corresponding positive document and serves as a crucial clue for answering the question. This finding\nindicates that our end-to-end optimization enables the query reasoner to implicitly encode reasoning-relevant\nknowledgealignedwiththegoldevidence,thusenhancingretrievalaccuracyandsemanticalignmentcompared\nto baseline systems.\n6", "meta": {"page": 6, "source": "2511.18659v2.pdf"}}
{"text": "Analysis of Decoded Tokens from Query Reasoner via Logit Lens\nQuestion: HowmanyyardsdidthenephewofIvoryLeeBrowngetduringhis2004truefreshmanseason?\nReasoned topics from query representation: Truly,Nep,IV,four,yards,NFL,Oklahoma,Ned,Neil,Howard,Kentucky...\nRetrieved Documents:\n[1]...AdrianLewisPeterson(bornMarch21,1985)isanAmericanfootballrunningbackfortheNewOrleansSaintsoftheNational\nFootballLeague(NFL).HeplayedcollegefootballatOklahomaandwasdraftedbytheMinnesotaVikingsseventhoverallinthe\n2007NFLDraft. PetersonsettheNCAAfreshmanrushingrecordwith1,925yardsasatruefreshmanduringthe2004season...\n[2]...IvoryLeeBrown(bornAugust17,1969)isaformerprofessionalAmericanfootballrunningbackintheNationalFootball\nLeagueandWorldLeagueofAmericanFootball. HeplayedforthePhoenixCardinalsoftheNFLandtheSanAntonioRidersof\ntheWLAF.BrownistheuncleofMinnesotaVikingsrunningbackAdrianPeterson...\n...\nAnswer: 1,925 yards\nTable1 Analysis of Decoded Tokens from Query Reasoner via Logit Lens. The highlighted tokens (red) denote the new\ninformation reasoned by the query reasoner, while (blue) denotes key evidence for solving this multihop task.\nModels CR NQ HotpotQA Musique 2Wiki Average\nNormal\nAutocompressor 1x 17.24 14.61 3.81 19.89 13.89\nMistral-7Bw/oBGEretrieval 1x 35.01 27.55 5.38 38.45 26.6\nMistral-7Bw/BGEretrieval 1x 54.58 42.94 8.94 44.24 37.67\nPhi4-miniw/oBGEretrieval 1x 18.77 21.10 4.05 30.26 18.55\nPhi4-miniw/BGEretrieval 1x 48.14 37.78 8.11 35.11 32.28\nllmlingua-2 4x 47.53 37.05 9.02 44.35 34.49\nSCP-Mistral-7B 4x 57.05+9.52 45.09+8.04 10.34+1.32 46.94+2.59 39.86+5.37\nSCP-Phi4-mini 4x 53.31+5.78 42.36+5.31 8.73-0.29 45.22+0.87 37.40+2.91\ncoconum 16x 24.12 21.48 3.52 24.48 18.40\npcc 16x 31.38 22.29 3.43 19.47 19.14\npisco 16x 54.39 41.94 10.09 44.88 37.83\nSCP-Mistral-7B 16x 55.56+1.17 43.72+1.78 10.55+0.46 46.00+1.12 38.96+1.13\nSCP-Phi4-mini 16x 51.96-2.43 40.86-1.08 8.61-1.48 44.27-0.61 36.42-1.42\nxrag 128x 32.35 25.16 3.64 28.79 22.48\nSCP-Mistral-7B 128x 53.36+21.01 41.37+16.21 10.26+6.62 46.40+17.61 37.85+15.37\nSCP-Phi4-mini 128x 43.09+10.74 33.92+8.76 6.87+3.23 43.70+14.91 31.90+9.42\nOracle\nAutocompressor 1x 29.47 19.24 7.16 26.74 20.65\nMistral-7Bw/BGEretrieval 1x 71.64 70.77 45.72 68.83 64.24\nPhi4-miniw/BGEretrieval 1x 66.10 64.06 37.07 52.69 54.98\nllmlingua-2 4x 63.99 52.42 27.47 53.92 49.45\nSCP-Mistral-7B 4x 76.50+12.51 73.81+21.39 46.26+18.79 70.48+16.56 66.76+17.31\nSCP-Phi4-mini 4x 73.67+9.68 72.41+19.99 40.13+12.66 64.22+10.30 62.61+13.16\ncoconum 16x 25.61 21.72 3.64 24.63 18.90\npcc 16x 49.62 34.56 18.25 27.56 32.50\npisco 16x 73.44 66.53 33.80 60.45 58.55\nSCP-Mistral-7B 16x 75.48+2.04 70.79+4.26 43.15+9.35 66.16+5.71 63.90+5.35\nSCP-Phi4-mini 16x 73.17-0.27 70.26+3.73 38.39+4.59 63.15+2.70 61.24+2.69\nxrag 128x 42.60 30.21 7.03 30.94 27.70\nSCP-Mistral-7B 128x 69.96+27.36 62.09+31.88 30.86+23.83 59.08+28.14 55.50+27.80\nSCP-Phi4-mini 128x 60.44+17.84 51.52+21.31 19.28+12.25 50.29+19.35 45.38+17.68\nTable2 Compressor performance on four QA datasets. The best performance is highlighted in bold. We show the\nabsolute performance change (¬±) of our method under different compression rates relative to its corresponding best\nbaseline performance. CR denotes compression rate.\n4 Experiments\n4.1 Experimentalsetup\nFollowing prior work Shi et al. (2025), we evaluate both the compressor and the end-to-end\nDatasets\nframeworkonthefulldevelopmentsetsoffourwidelyusedquestionansweringbenchmarks: NQ(Kwiatkowski\net al., 2019), HotpotQA (Yang et al., 2018), MuSiQue (Trivedi et al., 2022), and 2WikiMultihopQA (Ho\net al., 2020).\n7", "meta": {"page": 7, "source": "2511.18659v2.pdf"}}
{"text": "For compressor evaluation, we benchmark against both classical and recent methods, including\nBaselines\nAutoCompressor (Chevalier et al., 2023), XRAG(Cheng et al., 2025), COCOM (Rau et al., 2025), PCC\n(Dai et al., 2025), LLMLingual-2 (Pan et al., 2024), and PISCO (Louis et al., 2025a). For reranking,\nwe compare with BM25, BGE-Reranker (Chen et al., 2023), RankZephyr-7B (Pradeep et al., 2023),\nSetwise (Zhuang et al., 2024), and Rank-R1 (Zhuang et al., 2025). End-to-end QA results are evaluated\nagainst representative RAG systems, including prompt-based (GenGround (Shi et al., 2024), In-Context\nRAG), retrieval-optimized (ReComp (Xu et al., 2024), DPA-RAG (Dong et al., 2025)), fine-tuned LLMs\n(Self-RAG (Asai et al., 2024), Retrobust (Yoran et al., 2024), ChatQA (Liu et al., 2025)), and jointly\noptimized models (DDR-RAG (Li et al., 2024a), DRO (Shi et al., 2025)). Unlike all baselines operating\non raw text, our method is the first to jointly optimize reranking and generation directly over compressed\nrepresentations. Full experimental settings are provided in Appendix B. Below, we summarize the key\nfindings, while the complete set of additional experiments can be found in the Appendix, including pretraining\ndata analysis (App.C & D), training process analysis (App.E), fidelity and grounding evaluations (App.F), as\nwell as further module analyses (App.G).\n4.2 EvaluationofCompressionEffectiveness\nWe evaluate our document compressor under two settings: Normal and Oracle. In the Normal setting,\nthe model retrieves the top-5 documents from Wikipedia-2021 for each query. In the Oracle setting, the\nannotated positive document is included among the top-5 to isolate compression quality from retrieval noise.\nTable 2 summarizes results across compression ratios. For full results, please refer to table 6 Our method con-\nsistently outperforms all baselines. Compared to the best soft compression model PISCO, our model achieves\naverage gains of 1.13% (Normal) and 5.35% (Oracle); over the hard compression baseline LLMLingual-2,\nimprovements reach 5.37% and 17.31%, highlighting stronger semantic preservation.\nSurprisingly, our model exceeds the text-based w/ BGE retrieval baseline using uncompressed documents,\nwith average gains of 2.36% on Mistral-7B and 6.36% on Phi-4-min. This implies that well-trained\nsoft compression can retain essential reasoning information while substantially reducing input length. This\nmay be because the compressed representations filter out irrelevant content and focus the generator on the\nreasoning-relevant context, leading to better generalization than raw text inputs. While performance declines\nat extreme compression (beyond 32√ó in Oracle), the drop remains moderate under Normal conditions due to\nweaker document relevance.\n4.3 JointTrainingResults\nForend-to-endlearning,weevaluateourmodelunderbothNormalandOraclesettings. IntheNormalsetup,\neach query retrieves the top-20 documents from Wikipedia-2021; the Oracle setup adds annotated positives\nto the 20-document pool to isolate generation quality from retrieval noise. We compare two initialization\nstrategies for joint reranking‚Äìgeneration training: (i) from the compression pretraining checkpoint, and (ii)\nfrom the instruction-tuned compressor. Results are shown in Table 3, with full results in Table 7 in Appendix.\nUnder the Normal setting, performance remains stable across compression ratios, peaking at 16‚Äì32√ó. As\n4√ó might be harder to optim w/ NTP, CLaRa-Mistral-7B with 16x surpasses the text-based DRO-\nMistral-7B, improving F1 from 51.01‚Üí51.41 on NQ and 43.65‚Üí47.18 on 2Wiki. In the Oracle setting,\nperformance rises notably‚ÄîF1 exceeds 75% on both NQ and HotpotQA‚Äîshowing that joint optimization\neffectively exploits accurate retrieval.\nInstruction-tunedinitializationoutperformspretraining-basedinitializationunderNormalconditions,especially\non NQ and HotpotQA, indicating stronger alignment between compression and answering. However, the gap\nnarrows in the Oracle setting, suggesting initialization matters less when retrieval is reliable. Overall, CLaRa\ndemonstrates robust and scalable performance across retrieval qualities and compression ratios.\n4.4 Retrievalperformance\nWe evaluate our method on the document reranking task to assess retrieval effectiveness under the Oracle\nsetting, where positive documents are guaranteed in the candidate set, allowing accurate computation of\n8", "meta": {"page": 8, "source": "2511.18659v2.pdf"}}
{"text": "NQ HotpotQA Musique 2Wiki Average\nModels CR\nF1 EM F1 EM F1 EM F1 EM F1 EM\nPrompting-based Method\nGenGround* 1x 42.31 40.60 44.71 41.27 24.36 20.77 42.58 39.61 38.49 35.56\nIn-contextRAG* 1x 44.69 38.07 41.27 37.14 20.11 16.78 41.02 38.51 36.77 32.62\nRetrieval tuning\nRECOMP* 1x 42.67 37.47 42.72 38.72 24.96 17.34 38.26 32.17 37.15 31.43\nDPA-RAG* 1x 44.31 37.29 40.53 37.15 20.36 18.45 39.66 39.02 36.22 32.98\nLLM Fine-tuning\nRetRobust* 1x 43.82 37.03 40.54 35.59 18.16 18.11 39.11 38.65 35.41 32.34\nChatQA* 1x 34.54 23.64 44.60 33.40 17.05 16.64 31.90 26.80 32.02 25.12\nSelf-RAG* 1x 31.63 29.74 27.30 16.30 21.50 9.43 27.33 23.52 26.94 19.75\nEnd-to-end optimization\nDDR-RAG* 1x 28.76 40.74 35.44 31.71 10.57 13.54 38.40 35.44 28.29 30.36\nDRO-Mistral-7B* 1x 51.01 42.41 47.87 40.37 25.32 21.36 43.65 42.12 41.96 36.56\n4x 40.62 31.21 39.53 29.54 14.53 6.16 42.59 38.49 34.32 26.35\nCLaRa-Mistral-7B\n16x 41.75 32.24 44.37 33.72 15.36 6.99 43.47 39.50 36.24 28.11\n(Pretrianing-initialized)\n32x 40.68 31.36 41.84 31.26 15.32 6.66 43.23 38.98 35.27 27.06\n4x 48.21 38.16 45.93 35.12 17.49 8.11 47.18 43.11 39.70 31.12\nCLaRa-Mistral-7B\n16x 50.89 41.02 47.62 36.67 18.01 8.44 44.66 40.48 40.30 31.65\n(Instruction-initialized)\n32x 49.72 39.88 45.73 34.85 16.83 7.82 42.57 38.41 38.71 30.24\nOracle data setting\n4x 77.80 70.52 77.66 64.83 41.59 30.33 73.20 69.14 67.56 58.70\nCLaRa-Mistral-7B\n16x 73.81 65.74 69.57 56.76 31.15 21.18 65.90 61.31 60.11 51.25\n(Pretrianing-initialized)\n32x 72.03 63.65 70.91 57.07 33.40 22.22 66.32 61.12 60.66 51.02\n4x 75.63 67.64 69.66 56.92 33.19 22.42 73.86 69.74 63.08 54.18\nCLaRa-Mistral-7B\n16x 71.54 63.29 71.17 57.54 30.77 20.56 60.37 55.73 58.46 49.28\n(Instruction-initialized)\n32x 69.75 65.17 68.87 55.20 28.87 18.45 64.38 59.32 57.97 49.53\nTable3 End-to-End QA Performance. * indicates results reported from the DRO paper. CR denotes compression\nrate. The highest scores are shown in bold, and the second-best ones are underlined. Overall, our method achieves\ncomparable performance while reducing the required context length by 16√ó.\nRecall@k.. To compare supervision levels, we introduce a fully supervised retriever baseline, Sup-Instruct,\nwhich fine-tunes the Query Reasoner via contrastive learning with annotated positive and negative documents.\nIn contrast, our method trains the retriever in a weakly supervised manner, only using the next token\nprediction loss fromthe downstream generation. Notably, our method does not rely on any supervised\ndata of annotated document relevance labels.\nAs shown in Fig.4 (full results in Table8), CLaRa-Mistral-7B initialized from pretraining consistently\noutperformsitsinstruction-tunedversion,indicatingthatinstructiontuning,whileimprovinganswergeneration,\nbiases the model toward localized evidence at the cost of global semantics crucial for retrieval.\nRemarkably, under the pretraining-initialized setup, CLaRa even surpasses the fully supervised Sup-\nInstruct using ground-truth relevance labels. On HotpotQA (compression ratio 4), it achieves a Recall@5\nof 96.21%, exceeding the strongest supervised baseline BGE-Reranker (85.93%) by +10.28%. Despite\nrelyingsolelyonweakgenerationsupervision,CLaRapresumablycapturesdeepsemanticcorrelationsbetween\nqueries and documents and adapts to the downstream scenarios, achieving retrieval quality on par with or\nsurpassing fully supervised models.\n5 AblationStudy\nEachdocumentinoursetupispairedwithtwooutputtypes: (i)QA-stylequestion‚Äìanswer\nPretrainingDataMix\npairsand(ii)paraphraseddocuments. Toassesstheimpactofdatacomposition,wevarypretrainingobjectives\nand report results in Table 4 and 20. For both Mistral-7B and Phi4-mini, using either SimpleQA or Para-\nphrase alone already outperforms the no-pretraining baseline, showing that factual reasoning and paraphrastic\nrewriting both enrich compressed representations. Combining multiple QA types (SimpleQA+ComplexQA)\nor adding paraphrases (SimpleQA+ComplexQA+Para) achieves the best performance, confirming that di-\nverse objectives enhance semantic coverage and generalization‚Äîespecially under the Oracle setting, where\nhigh-quality retrieval amplifies pretraining benefits.\n9", "meta": {"page": 9, "source": "2511.18659v2.pdf"}}
{"text": "Figure4 Retrieval performance (Recall@1/3/5) on the Mistral-7B model across different reranking methods under\ncompression ratios = 4 and various initialization settings on NQ and HotpotQA datasets. Sup- denotes models\ntrainedwithlabeleddatausingcontrastivelearningforthereranker. -Pretraindenotesexperimentsconductedusing\nthe model checkpoint obtained after pretraining, while -Instruct denotes experiments conducted using the model\ncheckpoint obtained after instruction tuning.\nModels Datacomposition NQ HotpotQA Musique 2Wikiqa Average\nOracle\nNo-pretrain 70.01 61.13 29.00 57.43 54.39\nSimpleQA 72.66+2.65 66.41+5.28 35.29+6.29 61.22+3.79 58.90+4.51\nMistral-7B Para 73.86+3.85 68.64+7.51 36.86+7.86 63.22+5.79 60.64+6.25\nSimpleQA+ComplexQA 74.34+4.33 69.31+8.18 36.70+7.70 63.71+6.28 61.02+6.63\nSimpleQA+ComplexQA+Para 73.77+3.76 69.51+8.38 38.31+9.31 64.54+7.11 61.53+7.14\nNo-pretrain 65.54 60.32 27.31 56.39 52.39\nSimpleQA 68.70+3.16 64.60+4.28 30.41+3.10 57.46+1.07 55.29+2.90\nPhi4-mini Para 67.90+2.36 64.72+4.40 31.11+3.80 58.67+2.28 55.60+3.21\nSimpleQA+ComplexQA 69.33+3.79 65.15+4.83 31.15+3.84 57.94+1.55 55.89+3.50\nSimpleQA+ComplexQA+Para 69.90+4.36 65.32+5.00 31.77+4.46 58.52+2.13 56.38+3.99\nTable4 Effect of pretraining data composition on instruction-tuning performance under Oracle (gold context) settings\nunder the 32 compression ratio. We report the absolute score change (¬±) for each pretraining data setting relative to\nthe No-Pretrain baseline.\nWe analyze the effect of the mean-squared error (MSE) loss in Eq.2.2, which aligns\nEffectofMSELoss\ncompressed and original document representations. As shown in Table 5 and 21, the improvements are modest\n(0.3‚Äì0.6 points on average) but consistent across datasets, confirming that the MSE loss facilitates semantic\npreservation during compression. To provide a qualitative perspective, we visualize 4K document embeddings\nand their corresponding compressed representations using t-SNE (Figure 8 in Appendix). Without the MSE\nloss, the two distributions are clearly separated, reflecting a weak correspondence between the memory-token\nand document spaces. When the MSE loss is applied, the compressed embeddings exhibit strong overlap\nwith the original document representations, demonstrating that the alignment objective effectively enforces\nsemantic consistency between embedding spaces.\nModels CR NQ HotpotQA Musique 2Wikiqa Average\nOracle\nMistral-7B 32x 74.65 69.05 37.32 62.98 61.00\nw/mse 32x 73.77-0.88 69.51+0.46 38.31+0.99 64.54+1.56 61.53+0.53\nMistral-7B 128x 71.24 62.26 29.29 57.87 55.16\nw/mse 128x 69.96-1.28 62.09-0.17 30.86+1.57 59.08+1.21 55.50+0.34\nTable5 Instruction-tuning performance with and without the MSE loss under different compression ratios (CR = 32,\n128) and oracle retrieval settings.\n10", "meta": {"page": 10, "source": "2511.18659v2.pdf"}}
{"text": "6 RelatedWork\n6.1 Embedding-based/SoftCompression\nRecent studies have leveraged LLMs to compress lengthy RAG documents into continuous embeddings\nfor QA tasks (Chevalier et al., 2023; Ge et al., 2024; Mu et al., 2023; Xiao et al., 2025; Dai et al., 2025;\nKuratov et al., 2025). Generally, they shorten contexts using continuous representations but are trained\nindependently of LLMs and do not support retrieval‚Äìgeneration co-optimization. Cheng et al. (2025) propose\na projection module mapping each document to a single-vector representation while freezing encoder and\ndecoder parameters, achieving high compression but losing fine-grained semantics essential for RAG. Louis\net al. (2025a) introduce PISCO, which replaces documents with variable memory-token representations and\njointly trains the encoder and decoder for tighter coupling between compression and generation. While they\nsuggest pretraining offers limited gains with sufficient instruction data, our results show a more targeted\npretraining objective can still yield richer and more informative representations. The most related work, Louis\net al. (2025b), jointly trains a query-aware compression model that also functions as a retriever. However,\nrequiring re-compression per query contradicts the goal of reusable, query-independent representations and\nincreases latency. In contrast, our approach enables efficient, fully label-free retriever learning.\n6.2 End-to-EndOptimizationforRetrievalandGeneration\nReinforcement learning approaches (Shi et al., 2025) allow joint optimization but are unstable and\ncomputationally heavy, still relying on raw text. Differentiable reranking (Huang et al., 2025) enables\ngradient-based selection via Gumbel-softmax but likewise processes full documents at every step, leaving the\nrepresentation mismatch and context length issues unresolved.\nAs motivated earlier, joint training of retrieval and generation in RAG systems is hindered by the non-\ndifferentiability of discrete document selection. In typical QA pipelines, the retriever reorders retrieved\ndocuments before generation (Yu et al., 2024; Dong et al., 2024), but discrete sampling operations prevent\ngradient backpropagation. In contrast, our framework uniquely combines compression and joint training: by\nemploying length-flexible compressed vectors in a shared latent space, we enable efficient differentiable\nselectionwhiledrasticallyreducingcontextlength. Optimizedsolelythroughthegenerator‚Äôslanguagemodeling\nloss, CLaRa ensures consistent training‚Äìinference alignment and efficient end-to-end learning without explicit\nretrieval supervision.\n7 Conclusion\nIn this paper, we address the challenge of compressing documents into high-quality implicit representations to\nenhancetheperformanceofretrieval-augmentedgeneration(RAG)systemsthatrelyondocumentembeddings\nfor question answering. To this end, we design multiple pretraining objectives that leverage LLM prompting\nto construct diverse supervision signals, including QA pairs‚Äîcovering both simple and compositional reason-\ning‚Äîand paraphrased documents, encouraging the compressor to retain essential semantic information. We\nfurther introduce an efficient end-to-end training framework that unifies document representations across\nthe reranking and generation stages, leading to substantial improvements in retrieval accuracy and answer\nquality. Extensive experiments on multiple QA benchmarks demonstrate that embedding-based contextual\ncompression not only reduces input length and computation cost but also bridges the gap between retrieval\nand generation, enabling a more unified and semantically coherent RAG paradigm.\nLimitations\nThe current compressor is pretrained exclusively on Wikipedia data. To improve\nCompressorGeneralization.\ngeneralizability,futureworkmayincorporatedomain-adaptivepretrainingobjectivesandleveragemorediverse\ncorpora (e.g., code datasets (Wang et al., 2025)) to enhance representation robustness across modalities and\ndomains.\n11", "meta": {"page": 11, "source": "2511.18659v2.pdf"}}
{"text": "While our study does not primarily focuses on reasoning based\nReasoningoverCompressedRepresentations.\non compressed representations, the compact and semantically dense nature of compressed representations\nmakes them a promising candidate for integration into reasoning-oriented or agentic RAG frameworks, such as\nSearch-R1 (Jin et al., 2025). A natural next step is to investigate whether such representations can function\nas efficient reasoning memory in multi-hop or planning-based RAG systems.\nOur experiments are conducted on medium-scale models (Mistral-7B and Phi-4B). Larger models\nModelSize.\nmayproducehigher-qualitydocumentrepresentationsthatbettercapturesemanticnuances. Anopenquestion\nfor future work is whether there exists a model size threshold beyond which compressed representations\nsurpass raw text in supporting understanding and generation.\nThis work focuses on leveraging unified representations for both\nGeneralizationofImplicitRepresentations.\nunderstanding and generation within the RAG framework. Given the growing interest toward unified models\nthatjointlyperformcomprehensionandgeneration(Zhangetal.,2025),extendingcompression-basedmethods\nto broader tasks‚Äîsuch as tool learning (Qu et al., 2025)‚Äîoffers a promising direction for developing more\ngeneral-purpose, reasoning-capable systems. Besides, linking implicit understanding with implicit reasoning\n(Hao et al., 2025; Kang et al., 2025) would be an interesting direction.\nReferences\nMohammad Mahdi Abootorabi, Amirhosein Zobeiri, Mahdi Dehghani, Mohammadali Mohammadkhani, Bardia\nMohammadi, Omid Ghahroodi, Mahdieh Soleymani Baghshah, and Ehsaneddin Asgari. Ask in any modality: A\ncomprehensive survey on multimodal retrieval-augmented generation. In Wanxiang Che, Joyce Nabende, Ekaterina\nShutova,andMohammadTaherPilehvar(eds.),FindingsoftheAssociationforComputationalLinguistics: ACL2025,\npp. 16776‚Äì16809, Vienna, Austria, July 2025. Association for Computational Linguistics. ISBN 979-8-89176-256-5.\ndoi: 10.18653/v1/2025.findings-acl.861. URL https://aclanthology.org/2025.findings-acl.861/.\nAkari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. Self-RAG: Learning to retrieve, generate,\nand critique through self-reflection. In The Twelfth International Conference on Learning Representations, 2024.\nhttps://openreview.net/forum?id=hSyW5go0v8.\nOrlando Ayala and Patrice Bechard. Reducing hallucination in structured outputs via retrieval-augmented generation.\nIn Yi Yang, Aida Davani, Avi Sil, and Anoop Kumar (eds.), Proceedings of the 2024 Conference of the North\nAmerican Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 6:\nIndustry Track), pp. 228‚Äì238, Mexico City, Mexico, June 2024. Association for Computational Linguistics. doi:\n10.18653/v1/2024.naacl-industry.19. URL https://aclanthology.org/2024.naacl-industry.19/.\nJianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng Liu. Bge m3-embedding: Multi-lingual,\nmulti-functionality, multi-granularity text embeddings through self-knowledge distillation, 2023.\nXin Cheng, Xun Wang, Xingxing Zhang, Tao Ge, Si-Qing Chen, Furu Wei, Huishuai Zhang, and Dongyan Zhao.\nxrag: extreme context compression for retrieval-augmented generation with one token. In Proceedings of the 38th\nInternational Conference on Neural Information Processing Systems, NIPS ‚Äô24, Red Hook, NY, USA, 2025. Curran\nAssociates Inc. ISBN 9798331314385.\nAlexis Chevalier, Alexander Wettig, Anirudh Ajith, and Danqi Chen. Adapting language models to compress contexts.\nIn Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in\nNatural Language Processing, pp. 3829‚Äì3846, Singapore, December 2023. Association for Computational Linguistics.\ndoi: 10.18653/v1/2023.emnlp-main.232. URL https://aclanthology.org/2023.emnlp-main.232/.\nYuhong Dai, Jianxun Lian, Yitian Huang, Wei Zhang, Mingyang Zhou, Mingqi Wu, Xing Xie, and Hao Liao.\nPretraining context compressor for large language models with embedding-based memory. In Wanxiang Che, Joyce\nNabende, Ekaterina Shutova, and Mohammad Taher Pilehvar (eds.), Proceedings of the 63rd Annual Meeting of the\nAssociation for Computational Linguistics (Volume 1: Long Papers), pp. 28715‚Äì28732, Vienna, Austria, July 2025.\nAssociation for Computational Linguistics. ISBN 979-8-89176-251-0. doi: 10.18653/v1/2025.acl-long.1394. URL\nhttps://aclanthology.org/2025.acl-long.1394/.\nGuantingDong,YutaoZhu,ChenghaoZhang,ZechenWang,Ji-RongWen,andZhichengDou. UnderstandwhatLLM\nneeds: Dual preference alignment for retrieval-augmented generation. In THE WEB CONFERENCE 2025, 2025.\nhttps://openreview.net/forum?id=2ZaqnRIUCV.\n12", "meta": {"page": 12, "source": "2511.18659v2.pdf"}}
{"text": "Jialin Dong, Bahare Fatemi, Bryan Perozzi, Lin F. Yang, and Anton Tsitsulin. Don‚Äôt forget to connect! improving rag\nwith graph-based reranking, 2024. URL https://arxiv.org/abs/2405.18414.\nShahul Es, Jithin James, Luis Espinosa-Anke, and Steven Schockaert. Ragas: Automated evaluation of retrieval\naugmented generation, 2025. URL https://arxiv.org/abs/2309.15217.\nYunfanGao,YunXiong,XinyuGao,KangxiangJia,JinliuPan,YuxiBi,YiDai,JiaweiSun,MengWang,andHaofen\nWang. Retrieval-augmented generation for large language models: A survey, 2024. URL https://arxiv.org/abs/2312.10997.\nTao Ge, Hu Jing, Lei Wang, Xun Wang, Si-Qing Chen, and Furu Wei. In-context autoencoder for context com-\npression in a large language model. In The Twelfth International Conference on Learning Representations, 2024.\nhttps://openreview.net/forum?id=uREj4ZuGJE.\nShibo Hao, Sainbayar Sukhbaatar, DiJia Su, Xian Li, Zhiting Hu, Jason E Weston, and Yuandong Tian. Training\nlarge language model to reason in a continuous latent space. 2025. https://openreview.net/forum?id=tG4SgayTtk.\nXanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. Constructing a multi-hop QA dataset for\ncomprehensive evaluation of reasoning steps. In Donia Scott, Nuria Bel, and Chengqing Zong (eds.), Proceedings\nof the 28th International Conference on Computational Linguistics, pp. 6609‚Äì6625, Barcelona, Spain (Online),\nDecember 2020. International Committee on Computational Linguistics. doi: 10.18653/v1/2020.coling-main.580.\nURL https://aclanthology.org/2020.coling-main.580/.\nSiyuanHuang,ZhiyuanMa,JintaoDu,ChanghuaMeng,WeiqiangWang,JingwenLeng,MinyiGuo,andZhouhanLin.\nGumbel reranking: Differentiable end-to-end reranker optimization. In Wanxiang Che, Joyce Nabende, Ekaterina\nShutova, and Mohammad Taher Pilehvar (eds.), Proceedings of the 63rd Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers), pp. 7142‚Äì7161, Vienna, Austria, July 2025. Association for\nComputational Linguistics. ISBN 979-8-89176-251-0. doi: 10.18653/v1/2025.acl-long.354. URL https://aclanthology.org/\n2025.acl-long.354/.\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand\nJoulin,SebastianRiedel,andEdouardGrave. Atlas: few-shotlearningwithretrievalaugmentedlanguagemodels. J.\nMach. Learn. Res., 24(1), January 2023. ISSN 1532-4435.\nBowen Jin, Hansi Zeng, Zhenrui Yue, Jinsung Yoon, Sercan O Arik, Dong Wang, Hamed Zamani, and Jiawei Han.\nSearch-r1: Training LLMs to reason and leverage search engines with reinforcement learning. In Second Conference\non Language Modeling, 2025. https://openreview.net/forum?id=Rwhi91ideu.\nHaoqiang Kang, Yizhe Zhang, Nikki Lijing Kuang, Nicklas Majamaki, Navdeep Jaitly, Yi-An Ma, and Lianhui Qin.\nLadir: Latent diffusion enhances llms for text reasoning, 2025. URL https://arxiv.org/abs/2510.04573.\nYuriKuratov,MikhailArkhipov,AydarBulatov,andMikhailBurtsev. Cramming1568tokensintoasinglevectorand\nbackagain: Exploringthelimitsofembeddingspacecapacity. InWanxiangChe,JoyceNabende,EkaterinaShutova,\nandMohammadTaherPilehvar(eds.),Proceedings of the 63rd Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pp. 19323‚Äì19339, Vienna, Austria, July 2025. Association for Computational\nLinguistics.ISBN979-8-89176-251-0.doi: 10.18653/v1/2025.acl-long.948.URLhttps://aclanthology.org/2025.acl-long.948/.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle\nEpstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-\nWei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural questions: A benchmark for\nquestion answering research. Transactions of the Association for Computational Linguistics, 7:452‚Äì466, 2019. doi:\n10.1162/tacl_a_00276. URL https://aclanthology.org/Q19-1026/.\nKwun Hang Lau, Ruiyuan Zhang, Weijie Shi, Xiaofang Zhou, and Xiaojun Cheng. Reading between the timelines:\nRag for answering diachronic questions, 2025. URL https://arxiv.org/abs/2507.22917.\nQuinn Leng, Jacob Portes, Sam Havens, Matei Zaharia, and Michael Carbin. Long context rag performance of large\nlanguage models, 2024. URL https://arxiv.org/abs/2411.03538.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K√ºttler,\nMikeLewis,Wen-tauYih,TimRockt√§schel,SebastianRiedel,andDouweKiela. Retrieval-augmentedgenerationfor\nknowledge-intensivenlptasks. InProceedings of the 34th International Conference on Neural Information Processing\nSystems, NIPS ‚Äô20, Red Hook, NY, USA, 2020. Curran Associates Inc. ISBN 9781713829546.\nXinze Li, Sen Mei, Zhenghao Liu, Yukun Yan, Shuo Wang, Shi Yu, Zheni Zeng, Hao Chen, Ge Yu, Zhiyuan Liu,\net al. Rag-ddr: Optimizing retrieval-augmented generation using differentiable data rewards. arXiv preprint\narXiv:2410.13509, 2024a.\n13", "meta": {"page": 13, "source": "2511.18659v2.pdf"}}
{"text": "Xinze Li, Sen Mei, Zhenghao Liu, Yukun Yan, Shuo Wang, Shi Yu, Zheni Zeng, Hao Chen, Ge Yu, Zhiyuan\nLiu, Maosong Sun, and Chenyan Xiong. RAG-DDR: Optimizing retrieval-augmented generation using dif-\nferentiable data rewards. In The Thirteenth International Conference on Learning Representations, 2025.\nhttps://openreview.net/forum?id=Pnktu2PBXD.\nZhuowan Li, Cheng Li, Mingyang Zhang, Qiaozhu Mei, and Michael Bendersky. Retrieval augmented generation or\nlong-context LLMs? a comprehensive study and hybrid approach. In Franck Dernoncourt, Daniel Preo≈£iuc-Pietro,\nand Anastasia Shimorina (eds.), Proceedings of the 2024 Conference on Empirical Methods in Natural Language\nProcessing: Industry Track, pp. 881‚Äì893, Miami, Florida, US, November 2024b. Association for Computational\nLinguistics. doi: 10.18653/v1/2024.emnlp-industry.66. URL https://aclanthology.org/2024.emnlp-industry.66/.\nXi Victoria Lin, Xilun Chen, Mingda Chen, Weijia Shi, Maria Lomeli, Richard James, Pedro Rodriguez,\nJacob Kahn, Gergely Szilvasy, Mike Lewis, Luke Zettlemoyer, and Wen tau Yih. RA-DIT: Retrieval-\naugmented dual instruction tuning. In The Twelfth International Conference on Learning Representations, 2024.\nhttps://openreview.net/forum?id=22OTbutug9.\nZihan Liu, Wei Ping, Rajarshi Roy, Peng Xu, Chankyu Lee, Mohammad Shoeybi, and Bryan Catanzaro. Chatqa:\nsurpassing gpt-4 on conversational qa and rag. In Proceedings of the 38th International Conference on Neural\nInformation Processing Systems,NIPS‚Äô24,RedHook,NY,USA,2025.CurranAssociatesInc. ISBN9798331314385.\nMaxime Louis, Herv√© D√©jean, and St√©phane Clinchant. PISCO: Pretty simple compression for retrieval-augmented\ngeneration. In Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar (eds.), Findings\nof the Association for Computational Linguistics: ACL 2025, pp. 15506‚Äì15521, Vienna, Austria, July 2025a.\nAssociation for Computational Linguistics. ISBN 979-8-89176-256-5. doi: 10.18653/v1/2025.findings-acl.800. URL\nhttps://aclanthology.org/2025.findings-acl.800/.\nMaximeLouis,ThibaultFormal,Herv√©Dejean,andSt√©phaneClinchant. Oscar: Onlinesoftcompressionandreranking,\n2025b. URL https://arxiv.org/abs/2504.07109.\nThomas Merth, Qichen Fu, Mohammad Rastegari, and Mahyar Najibi. Superposition prompting: improving and\naccelerating retrieval-augmented generation. In Proceedings of the 41st International Conference on Machine\nLearning, ICML‚Äô24. JMLR.org, 2024.\nJesse Mu, Xiang Lisa Li, and Noah Goodman. Learning to compress prompts with gist tokens. In Proceedings of the\n37th International Conference on Neural Information Processing Systems, NIPS ‚Äô23, Red Hook, NY, USA, 2023.\nCurran Associates Inc.\nnostalgebraist. interpreting GPT: the logit lens, Aug 2020. URL https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/\ninterpreting-gpt-the-logit-lens. LessWrong blog post.\nZhuoshi Pan, Qianhui Wu, Huiqiang Jiang, Menglin Xia, Xufang Luo, Jue Zhang, Qingwei Lin, Victor R√ºhle, Yuqing\nYang,Chin-YewLin,H.VickyZhao,LiliQiu,andDongmeiZhang. LLMLingua-2: Datadistillationforefficientand\nfaithful task-agnostic prompt compression. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Findings of\ntheAssociationforComputationalLinguistics: ACL2024,pp.963‚Äì981,Bangkok,Thailand,August2024.Association\nfor Computational Linguistics. doi: 10.18653/v1/2024.findings-acl.57. URL https://aclanthology.org/2024.findings-acl.57/.\nRonak Pradeep, Sahel Sharifymoghaddam, and Jimmy Lin. Rankzephyr: Effective and robust zero-shot listwise\nreranking is a breeze!, 2023. URL https://arxiv.org/abs/2312.02724.\nChangle Qu, Sunhao Dai, Xiaochi Wei, Hengyi Cai, Shuaiqiang Wang, Dawei Yin, Jun Xu, and Ji-rong Wen. Tool\nlearning with large language models: a survey. Front. Comput. Sci., 19(8), January 2025. ISSN 2095-2228. doi:\n10.1007/s11704-024-40678-2. URL https://doi.org/10.1007/s11704-024-40678-2.\nDavidRau,ShuaiWang,Herv√©D√©jean,St√©phaneClinchant,andJaapKamps. Contextembeddingsforefficientanswer\ngeneration in retrieval-augmented generation. In Proceedings of the Eighteenth ACM International Conference on\nWeb Search and Data Mining, WSDM ‚Äô25, pp. 493‚Äì502, New York, NY, USA, 2025. Association for Computing\nMachinery. ISBN 9798400713293. doi: 10.1145/3701551.3703527. URL https://doi.org/10.1145/3701551.3703527.\nDevendra Singh Sachan, Siva Reddy, William L. Hamilton, Chris Dyer, and Dani Yogatama. End-to-end\ntraining of multi-document reader and retriever for open-domain question answering. In A. Beygelzimer,\nY. Dauphin, P. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems,\n2021. https://openreview.net/forum?id=5KWmB6JePx.\nZhengliang Shi, Shuo Zhang, Weiwei Sun, Shen Gao, Pengjie Ren, Zhumin Chen, and Zhaochun Ren. Generate-then-\nground in retrieval-augmented generation for multi-hop question answering. In Lun-Wei Ku, Andre Martins, and\nVivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics\n14", "meta": {"page": 14, "source": "2511.18659v2.pdf"}}
{"text": "(Volume1: LongPapers),pp.7339‚Äì7353,Bangkok,Thailand,August2024.AssociationforComputationalLinguistics.\ndoi: 10.18653/v1/2024.acl-long.397. URL https://aclanthology.org/2024.acl-long.397/.\nZhengliangShi,LingyongYan,WeiweiSun,YueFeng,PengjieRen,XinyuMa,ShuaiqiangWang,DaweiYin,Maarten\ndeRijke,andZhaochunRen. Directretrieval-augmentedoptimization: Synergizingknowledgeselectionandlanguage\nmodels, 2025. URL https://arxiv.org/abs/2505.03075.\nHarsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. MuSiQue: Multihop questions via\nsingle-hop question composition. Transactions of the Association for Computational Linguistics, 10:539‚Äì554, 2022.\ndoi: 10.1162/tacl_a_00475. URL https://aclanthology.org/2022.tacl-1.31/.\nZora Zhiruo Wang, Akari Asai, Xinyan Velocity Yu, Frank F. Xu, Yiqing Xie, Graham Neubig, and Daniel Fried.\nCoderag-bench: Can retrieval augment code generation?, 2025. URL https://arxiv.org/abs/2406.14497.\nShangyu Wu, Ying Xiong, Yufei Cui, Haolun Wu, Can Chen, Ye Yuan, Lianming Huang, Xue Liu, Tei-Wei Kuo, Nan\nGuan, and Chun Jason Xue. Retrieval-augmented generation for natural language processing: A survey. CoRR,\nabs/2407.13193, 2024. URL https://doi.org/10.48550/arXiv.2407.13193.\nZilinXiao,QiMa,MengtingGu,ChunchengJasonChen,XintaoChen,VicenteOrdonez,andVijaiMohan.Metaembed:\nScaling multimodal retrieval at test-time with flexible late interaction, 2025. URL https://arxiv.org/abs/2509.18095.\nFangyuan Xu, Weijia Shi, and Eunsol Choi. RECOMP: Improving retrieval-augmented LMs with context compres-\nsion and selective augmentation. In The Twelfth International Conference on Learning Representations, 2024.\nhttps://openreview.net/forum?id=mlJLVigNHp.\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D.\nManning. HotpotQA:Adatasetfordiverse,explainablemulti-hopquestionanswering. InEllenRiloff,DavidChiang,\nJulia Hockenmaier, and Jun‚Äôichi Tsujii (eds.), Proceedings of the 2018 Conference on Empirical Methods in Natural\nLanguage Processing, pp. 2369‚Äì2380, Brussels, Belgium, October-November 2018. Association for Computational\nLinguistics. doi: 10.18653/v1/D18-1259. URL https://aclanthology.org/D18-1259/.\nOri Yoran, Tomer Wolfson, Ori Ram, and Jonathan Berant. Making retrieval-augmented language models\nrobust to irrelevant context. In The Twelfth International Conference on Learning Representations, 2024.\nhttps://openreview.net/forum?id=ZS4m74kZpH.\nYue Yu, Wei Ping, Zihan Liu, Boxin Wang, Jiaxuan You, Chao Zhang, Mohammad Shoeybi, and Bryan Catanzaro.\nRankRAG: Unifying context ranking with retrieval-augmented generation in LLMs. In The Thirty-eighth Annual\nConference on Neural Information Processing Systems, 2024. https://openreview.net/forum?id=S1fc92uemC.\nZhenrui Yue, Honglei Zhuang, Aijun Bai, Kai Hui, Rolf Jagerman, Hansi Zeng, Zhen Qin, Dong Wang, Xuanhui\nWang, and Michael Bendersky. Inference scaling for long-context retrieval augmented generation. In The Thirteenth\nInternational Conference on Learning Representations, 2025. https://openreview.net/forum?id=FSjIrOm1vz.\nTianyi Zhang*, Varsha Kishore*, Felix Wu*, Kilian Q. Weinberger, and Yoav Artzi. Bertscore: Eval-\nuating text generation with bert. In International Conference on Learning Representations, 2020.\nhttps://openreview.net/forum?id=SkeHuCVFDr.\nXinjie Zhang, Jintao Guo, Shanshan Zhao, Minghao Fu, Lunhao Duan, Jiakui Hu, Yong Xien Chng, Guo-Hua Wang,\nQing-GuoChen,ZhaoXu,WeihuaLuo,andKaifuZhang. Unifiedmultimodalunderstandingandgenerationmodels:\nAdvances, challenges, and opportunities, 2025. URL https://arxiv.org/abs/2505.02567.\nJiawei Zhou and Lei Chen. Optimizing retrieval for rag via reinforced contrastive learning, 2025. URL https:\n//arxiv.org/abs/2510.24652.\nShengyao Zhuang, Honglei Zhuang, Bevan Koopman, and Guido Zuccon. A setwise approach for effective and\nhighly efficient zero-shot ranking with large language models. In Proceedings of the 47th International ACM\nSIGIR Conference on Research and Development in Information Retrieval, SIGIR ‚Äô24, pp. 38‚Äì47, New York, NY,\nUSA, 2024. Association for Computing Machinery. ISBN 9798400704314. doi: 10.1145/3626772.3657813. URL\nhttps://doi.org/10.1145/3626772.3657813.\nShengyao Zhuang, Xueguang Ma, Bevan Koopman, Jimmy Lin, and Guido Zuccon. Rank-r1: Enhancing reasoning in\nllm-based document rerankers via reinforcement learning, 2025. URL https://arxiv.org/abs/2503.06034.\nWeronika ≈Åajewska, Momchil Hardalov, Laura Aina, Neha Anna John, Hang Su, and Llu√≠s M√†rquez. Understanding\nand improving information preservation in prompt compression for llms, 2025. URL https://arxiv.org/abs/2503.19114.\n15", "meta": {"page": 15, "source": "2511.18659v2.pdf"}}
{"text": "A GradientsforNon-sharedvs.SharedRepresentationsinRAG\nStep1: Let s xd be the retrieval score for query x and document d, and let\np(d|x)= exp(s xd ) , p(y|x)= (cid:88) p(d|x)p(y|x,d), L=‚àílogp(y|x). (A.1)\n(cid:80)\nexp(s )\nd‚Ä≤‚ààC xd‚Ä≤ d‚ààC\nStep2:productruleinsidethesum.\n‚àÇ (cid:16)(cid:88) (cid:17) (cid:88) ‚àÇp(d‚Ä≤|x) (cid:88) ‚àÇp(y|x,d‚Ä≤)\np(d‚Ä≤|x)p(y|x,d‚Ä≤) = p(y|x,d‚Ä≤)+ p(d‚Ä≤|x) . (A.2)\n‚àÇs ‚àÇs ‚àÇs\nxd xd xd\nd‚Ä≤ d‚Ä≤ (cid:124) (cid:123)(cid:122) (cid:125) d‚Ä≤\nsoftmaxJacobian\nStep3:softmaxJacobian. For p(d‚Ä≤|x)= (cid:80) esx e d s ‚Ä≤ xj ,\nj\n‚àÇp(d‚Ä≤|x)\n=p(d‚Ä≤|x) (cid:0) 1[d‚Ä≤ =d]‚àíp(d|x) (cid:1) .\n‚àÇs\nxd\nTherefore\n(cid:88)‚àÇp(d‚Ä≤|x) p(y|x,d‚Ä≤)= (cid:88) p(d‚Ä≤|x) (cid:0) 1[d‚Ä≤ =d]‚àíp(d|x) (cid:1) p(y|x,d‚Ä≤) (A.3)\n‚àÇs\nxd\nd‚Ä≤ d‚Ä≤\n(cid:88)\n=p(d|x)p(y|x,d)‚àíp(d|x) p(d‚Ä≤|x)p(y|x,d‚Ä≤) (A.4)\nd‚Ä≤\n=p(d|x) (cid:0) p(y|x,d)‚àíp(y|x) (cid:1) . (A.5)\nPlugging equation A.5 into equation A.2 and then equation A.1 gives\nStep4:puttogether.\nÔ£Æ Ô£π\n‚àÇL =‚àí 1 Ô£Ø Ô£Ø Ô£Øp(d|x) (cid:0) p(y|x,d)‚àíp(y|x) (cid:1) + (cid:88) p(d‚Ä≤|x) ‚àÇp(y|x,d‚Ä≤) Ô£∫ Ô£∫ Ô£∫. (A.6)\n‚àÇs p(y|x)Ô£Ø ‚àÇs Ô£∫\nxd Ô£∞(cid:124) (cid:123)(cid:122) (cid:125) d‚Ä≤ xd Ô£ª\n(I)probabilitypath (cid:124) (cid:123)(cid:122) (cid:125)\n(II)representation/generationpath\nStep5:commonsimplification(assumption). If the generator‚Äôs conditional p(y|x,d‚Ä≤) depends on s xd only when\nd‚Ä≤ =d (e.g., each conditional uses its own selected document; non-shared case gives it zero), then the second\nsum reduces to a single term:\n(cid:88) ‚àÇp(y|x,d‚Ä≤) ‚àÇp(y|x,d)\np(d‚Ä≤|x) = p(d|x) .\n‚àÇs ‚àÇs\nxd xd\nd‚Ä≤\nUnder this widely-used assumption, equation A.6 becomes\n‚àÇL =‚àí 1 (cid:104) p(d|x) (cid:0) p(y|x,d)‚àíp(y|x) (cid:1) + p(d|x) ‚àÇp(y|x,d) (cid:105) . (A.7)\n‚àÇs p(y|x) ‚àÇs\nxd (cid:124) (cid:123)(cid:122) (cid:125) xd\n(I)probabilitypath (cid:124) (cid:123)(cid:122) (cid:125)\n(II)representation/generationpath\nRemark(moregeneralshared-conditioning). If the generator conditions on a mixture r = (cid:80) j œÄ j (s)z j (so every\np(y|x,d‚Ä≤) shares the same r), then ‚àÇp(y|x,d‚Ä≤) is the same for all d‚Ä≤ and (cid:80) p(d‚Ä≤|x)‚àÇp(y|x,d‚Ä≤) = ‚àÇp(y|x,r). Both\nforms are consistent; the boxed formu ‚àÇ la sxdcorresponds to the per-documen d t ‚Ä≤ conditiona ‚àÇ l s vxdiew. ‚àÇsxd\nTerm (II) is present if the generator‚Äôs conditional p(y|x,d) depends (directly or indirectly) on s .\nxd\n16", "meta": {"page": 16, "source": "2511.18659v2.pdf"}}
{"text": "CaseA:Non-sharedrepresentations(retrieverÃ∏=generator)\nHere the generator consumes raw tokens or an independent encoder, hence p(y|x,d) does not depend on s :\nxd\n‚àÇp(y|x,d)\n=0. (A.8)\n‚àÇs\nxd\nPlugging into equation A.7 gives the complete gradient:\n‚àÇL =‚àí 1 p(d|x) (cid:0) p(y|x,d)‚àíp(y|x) (cid:1) . (A.9)\n‚àÇs p(y|x)\nxd\nThis expression already accounts for the softmax coupling via p(y|x) and is more accurate than writing only\n‚àí 1 p(y|x,d)‚àÇp(d|x)/‚àÇs .\np(y|x) xd\nCaseB:Sharedrepresentations(retriever=generator)\nWhenretrieverandgeneratorshareembeddings,p(y|x,d)dependsons throughthegenerator‚Äôsconditioning\nxd\nvector. A common differentiable conditioning is\nœÄ = exp(s xj /œÑ) , r = (cid:88) œÄ z , p(y|x,d)‚â°p(y|x,r), (A.10)\nj (cid:80) exp(s /œÑ) j j\n‚Ñì x‚Ñì j‚ààC\nwhere œÑ >0 is the temperature, z are document embeddings, and r is fed to the generator.\nj\nLet\ng ‚âú\n‚àÇlogp(y|x,r)\n=\n(cid:88)‚àÇlogp(y\nt\n|y\n<t\n,x,r)\n. (A.11)\n‚àÇr ‚àÇr\nt\nUsing the softmax Jacobian,\n‚àÇr = (cid:88) ‚àÇœÄ j z = 1 œÄ (z ‚àír). (A.12)\n‚àÇs ‚àÇs j œÑ d d\nxd xd\nj\nBy chain rule,\n‚àÇp(y|x,d) =p(y|x,r)g‚ä§ ‚àÇr = p(y|x,r) œÄ g‚ä§(cid:0) z ‚àír (cid:1) . (A.13)\n‚àÇs ‚àÇs œÑ d d\nxd xd\nSubstituting equation A.13 into equation A.7 yields the full shared-representation gradient:\n‚àÇL =‚àí 1 (cid:104) p(d|x) (cid:0) p(y|x,d)‚àíp(y|x) (cid:1) + p(r|x)p(y|x,r) œÄ g‚ä§(cid:0) z ‚àír (cid:1)(cid:105) . (A.14)\n‚àÇs p(y|x) œÑ d d\nxd\nStraight-through(ST)note.\nIf the forward pass uses hard top-k selection (argmax/indices) but the backward\npass adopts the softmax gradient (ST estimator), then formulas equation A.12‚Äìequation A.14 remain the\ncorrect backpropagation rules (with œÄ computed from the scores for the backward pass).\nOptional:Cosine-similarityscorebackpropagation\nIf the score is cosine similarity\nq‚ä§z\ns = d , (A.15)\nxd ‚à•q‚à•‚à•z ‚à•\nd\n17", "meta": {"page": 17, "source": "2511.18659v2.pdf"}}
{"text": "Algorithm 1 Differentiable Top-k Selection with Straight-Through Estimator in CLaRA\n1: Input: Similarityscoress‚ààRB√óD,temperatureœÑ,numberofselectionsk\n2: Output: SelectiontensorZ‚ààRB√ók√óD andtop-k indices{rj}k\nj=1\n3: sÀú‚Üês/max(œÑ,10‚àí6)\n4: InitializeZ ,Z ‚Üê0B√ók√óD, taken‚Üê0B√óD\nhard soft\n5: forj=1tok do\n6: (1) Hard selection: rj ‚ÜêargmaxisÀú(:,i)onunmaskedcandidates\n7: Z\nhard\n[:,j,rj]‚Üê1\n8: (2) Soft selection: mask‚Üê1‚àíSG(taken)\n9: logits ‚ÜêsÀú+log(mask+Œµ)\nj\n10: pj ‚Üêsoftmax(logits\nj\n)\n11: Z\nsoft\n[:,j,:]‚Üêpj\n12: taken‚Üêmin(taken+Z [:,j,:],1)\nhard\n13: endfor\n14: (3) Straight-through estimator: Z‚ÜêZ +(Z ‚àíSG(Z ))\nhard soft soft\n15: return(Z,{rj}k\nj=1\n)\nthen the required Jacobians are\n(cid:18) (cid:19) (cid:18) (cid:19)\n‚àÇs 1 q ‚àÇs 1 z\nxd = z ‚àís ‚à•z ‚à• , xd = q‚àís d ‚à•q‚à• . (A.16)\n‚àÇq ‚à•q‚à•‚à•z ‚à• d xd ‚à•q‚à•2 d ‚àÇz ‚à•q‚à•‚à•z ‚à• xd ‚à•z ‚à•2\nd d d d\nHence\n‚àÇL = (cid:88) ‚àÇL ‚àÇs xd, ‚àÇL = ‚àÇL ‚àÇs xd. (A.17)\n‚àÇq ‚àÇs ‚àÇq ‚àÇz ‚àÇs ‚àÇz\nxd d xd d\nd‚ààC\nSimple QA Complex QA Paraphrase Doc\nNum. 2,000,000 2,000,000 1,966,291\nAvg.pairs 7.80 4.62 1.00\nAvg.inp 95.56 95.56 95.56\nAvg.out 158.18 253.90 108.67\nTable9 Pretraining data statistics for SCP. The table reports the total number of training examples (Num.), average\nnumber of generated QA pairs or documents (Avg.pairs), average input document length (Avg.inp) and average\ngenerated text length (Avg.out) for Simple QA, Complex QA, and Paraphrased Documents..\nDatasets TrainingDataSize EvaluationDataSize\nNatureQuestion 58,622 6,489\nHotpotQA 90,185 7,384\nMusiQue 168,745 2,417\n2WikiMultiHopQA 167,454 12,576\nTable10 Statistics of experimental datasets.\nB Detailedexperimentalsetup\nB.1 Datasets\nThe pretraining corpus consists of 2M documents and their corresponding 2M SimpleQA sets, 2M ComplexQA\nsets, and 2M paraphrased documents. Detailed statistics on data composition and distribution are provided\nin Table 9.\nDuring the instruction tuning stage of compression learning, we use question data from COCOM (Rau\net al., 2025) , which contains 453k questions. We employ the Mistral-7B model and retrieve the top-5 most\n18", "meta": {"page": 18, "source": "2511.18659v2.pdf"}}
{"text": "Models CR NQ HotpotQA Musique 2Wiki Average\nNormal\nAutocompressor 1x 17.24 14.61 3.81 19.89 13.89\nxrag 128x 32.35 25.16 3.64 28.79 22.48\ncoconum 16x 24.12 21.48 3.52 24.48 18.40\npcc 16x 31.38 22.29 3.43 19.47 19.14\nllmlingua-2 4x 47.53 37.05 9.02 44.35 34.49\npisco 16x 54.39 41.94 10.09 44.88 37.83\nMistral-7Bw/oBGEretrieval 1x 35.01 27.55 5.38 38.45 26.6\nMistral-7Bw/BGEretrieval 1x 54.58 42.94 8.94 44.24 37.67\n4x 57.05+2.47 45.09+2.15 10.34+1.40 46.94+2.70 39.86+2.19\n16x 55.56+0.98 43.72+0.78 10.55+1.61 46.00+1.76 38.96+1.29\nSCP-Mistral-7B\n32x 54.64+0.06 43.52+0.58 10.55+1.61 46.58+2.34 38.82+1.15\n64x 54.18-0.40 42.17-0.77 10.17+1.23 47.03+2.79 38.39+0.72\n128x 53.36-1.22 41.37-1.57 10.26+1.32 46.40+2.16 37.85+0.18\n256x 52.84-1.74 40.00-2.94 10.38+1.44 46.31+2.07 37.38-0.29\nPhi4-miniw/oBGEretrieval 1x 18.77 21.10 4.05 30.26 18.55\nPhi4-miniw/BGEretrieval 1x 48.14 37.78 8.11 35.11 32.28\n4x 53.31+5.17 42.36+4.58 8.73+0.62 45.22+10.11 37.40+5.12\n16x 51.96+3.82 40.86+3.08 8.61+0.50 44.27+9.16 36.42+4.14\nSCP-Phi4-mini\n32x 49.30+1.16 38.62+0.84 7.70-0.41 43.71+8.60 34.83+2.55\n64x 45.72-2.42 35.75-2.03 6.50-1.61 43.96+8.85 32.98+0.70\n128x 43.09-5.05 33.92-3.86 6.87-1.24 43.70+8.59 31.90-0.38\n256x 42.73-5.41 34.02-3.76 6.87-1.24 43.75+8.64 31.84-0.44\nOracle\nAutocompressor 1x 29.47 19.24 7.16 26.74 20.65\nxrag 128x 42.60 30.21 7.03 30.94 27.70\ncoconum 16x 25.61 21.72 3.64 24.63 18.90\npcc 16x 49.62 34.56 18.25 27.56 32.50\nllmlingua-2 4x 63.99 52.42 27.47 53.92 49.45\npisco 16x 73.44 66.53 33.80 60.45 58.55\nMistral-7Bw/BGEretrieval 1x 71.64 70.77 45.72 68.83 64.24\n4x 76.50+4.86 73.81+3.04 46.26+0.54 70.48+1.65 66.76+2.52\n16x 75.48+3.84 70.79+0.02 43.15-2.57 66.16-2.67 63.90-0.34\nSCP-Mistral-7B\n32x 73.77+2.13 69.51-1.26 38.31-7.41 64.54-4.29 61.53-2.71\n64x 71.90+0.26 66.22-4.55 34.96-10.76 61.55-7.28 58.66-5.58\n128x 69.96-1.68 62.09-8.68 30.86-14.86 59.08-9.75 55.50-8.74\n256x 68.82-2.82 59.93-10.84 26.19-19.53 56.50-12.33 52.86-11.38\nPhi4-miniw/BGEretrieval 1x 66.10 64.06 37.07 52.69 54.98\n4x 73.67+7.57 72.41+8.35 40.13+3.06 64.22+11.53 62.61+7.63\n16x 73.17+7.07 70.26+6.20 38.39+1.32 63.15+10.46 61.24+6.26\nSCP-Phi4-mini\n32x 69.90+3.80 65.32+1.26 31.77-5.30 58.52+5.83 56.38+1.40\n64x 64.72-1.38 57.79-6.27 23.54-13.53 53.11+0.42 49.79-5.19\n128x 60.44-5.66 51.52-12.54 19.28-17.79 50.29-2.40 45.38-9.60\n256x 60.12-5.98 51.54-12.52 19.61-17.46 50.33-2.36 45.40-9.58\nTable6 Compressor performance on four QA datasets. The best performance is highlighted in bold. We show the\nabsolute performance change (¬±) of our method under different compression rates relative to its corresponding w/\nretrieval setting. CR denotes compression rate.\nsimilar documents from the Wikipedia-2021 corpus using dense retrieval. Given each query and its retrieved\ndocuments, the model is prompted to generate the corresponding answer, which serves as the gold target for\ninstruction tuning.\nFor end-to-end training, we use the training set of each benchmark individually, except for MuSiQue. Since\nMuSiQue is more challenging and difficult to converge when trained alone, we construct its training set by\ncombining the training samples from HotpotQA, 2Wiki, and MuSiQue. For each query, we first obtain its\npositive documents, and then retrieve additional documents from the corpus using the BGE-large-en-v1.5\nmodel until we collect a total of 20 candidates. This ensures that the gold answer remains inferable from at\nleast one of the selected documents during end-to-end optimization. Table 10 summarizes the data statistics.\n19", "meta": {"page": 19, "source": "2511.18659v2.pdf"}}
{"text": "NQ HotpotQA Musique 2Wiki\nModels CR RetrievalMode\nF1 EM F1 EM F1 EM F1 EM\nPrompting-based Method\nGenGround* 1x Normal 42.31 40.60 44.71 41.27 24.36 20.77 42.58 39.61\nIn-contextRAG* 1x Normal 44.69 38.07 41.27 37.14 20.11 16.78 41.02 38.51\nRetrieval tuning\nRECOMP* 1x Normal 42.67 37.47 42.72 38.72 24.96 17.34 38.26 32.17\nDPA-RAG* 1x Normal 44.31 37.29 40.53 37.15 20.36 18.45 39.66 39.02\nLLM Fine-tuning\nRetRobust* 1x Normal 43.82 37.03 40.54 35.59 18.16 18.11 39.11 38.65\nChatQA* 1x Normal 34.54 23.64 44.60 33.40 17.05 16.64 31.90 26.80\nSelf-RAG* 1x Normal 31.63 29.74 27.30 16.30 21.50 9.43 27.33 23.52\nEnd-to-end optimization\nDDR-RAG* 1x Normal 28.76 40.74 35.44 31.71 10.57 13.54 38.40 35.44\nDRO-Mistral-7B* 1x Normal 51.01 42.41 47.87 40.37 25.32 21.36 43.65 42.12\nPretraining-initialized\n4x 40.62 31.21 39.53 29.54 14.53 6.16 42.59 38.49\n16x 41.75 32.24 44.37 33.72 15.36 6.99 43.47 39.50\n32x 40.68 31.36 41.84 31.26 15.32 6.66 43.23 38.98\nNormal\n64x 41.58 31.38 41.62 31.12 14.78 6.16 42.64 38.40\n128x 42.04 31.78 42.26 31.78 15.53 6.37 41.80 37.37\n256x 42.90 32.58 41.32 30.44 15.44 6.41 41.96 37.60\nCLaRa-Mistral-7B\n4x 77.80 70.52 77.66 64.83 41.59 30.33 73.20 69.14\n16x 73.81 65.74 69.57 56.76 31.15 21.18 65.90 61.31\n32x 72.03 63.65 70.91 57.07 33.40 22.22 66.32 61.12\nOracle\n64x 68.18 59.56 67.64 53.22 28.43 17.42 62.53 57.02\n128x 68.66 59.25 66.51 52.30 28.44 16.67 64.82 58.97\n256x 66.85 57.17 63.43 49.08 27.44 16.92 62.96 57.35\n4x 39.69 30.41 37.10 27.33 15.20 6.08 38.43 34.26\n16x 31.93 23.38 37.21 27.22 14.30 4.84 40.03 35.62\n32x 30.70 21.78 37.14 26.99 13.26 4.39 38.15 33.82\nNormal\n64x 28.88 19.91 34.98 25.07 13.31 4.55 37.74 33.57\n128x 29.26 19.85 34.73 24.95 13.07 4.01 36.41 32.23\n256x 29.92 20.53 34.10 24.62 13.07 4.22 35.98 31.61\nCLaRa-Phi4-mini\n4x 61.07 52.15 59.82 46.99 25.87 15.76 56.84 52.12\n16x 65.09 55.96 57.87 45.26 21.09 11.25 55.75 50.41\n32x 62.35 52.07 51.06 38.33 20.98 10.92 50.68 45.41\nOracle\n64x 58.51 47.08 55.47 41.62 23.89 12.99 53.90 48.26\n128x 56.13 44.52 52.68 38.49 20.74 9.85 49.97 44.11\n256x 54.58 43.24 51.62 37.77 21.45 9.89 48.46 42.63\nInstruction-tuned-initialized\n4x 48.21 38.16 45.93 35.12 17.49 8.11 47.18 43.11\n16x 50.89 41.02 47.62 36.67 18.01 8.44 44.66 40.48\n32x 49.72 39.88 45.73 34.85 16.83 7.82 42.57 38.41\nNormal\n64x 50.91 41.07 45.68 34.74 16.76 7.65 40.34 35.91\n128x 51.41 41.27 44.63 33.88 15.75 7.03 40.55 36.12\n256x 50.57 40.39 43.02 32.26 16.02 6.99 40.10 35.77\nCLaRa-Mistral-7B\n4x 75.63 67.64 69.66 56.92 33.19 22.42 73.86 69.74\n16x 71.54 63.29 71.17 57.54 30.77 20.56 60.37 55.73\n32x 69.75 65.17 68.87 55.20 28.87 18.45 64.38 59.32\nOracle\n64x 68.17 59.04 66.64 52.87 27.30 16.96 60.98 55.59\n128x 66.95 57.61 64.09 54.63 26.11 15.97 62.34 56.64\n256x 65.60 55.65 61.79 47.74 27.67 17.05 59.40 53.75\n4x 41.86 31.96 39.44 29.32 15.70 5.59 37.63 33.40\n16x 42.17 32.61 42.77 32.00 15.84 6.08 36.69 32.47\n32x 39.14 29.45 42.59 31.47 15.55 5.71 41.47 36.68\nNormal\n64x 36.91 27.09 38.90 28.02 14.08 4.88 39.52 34.98\n128x 36.26 26.34 36.39 26.44 14.70 5.42 37.14 32.85\n256x 37.58 27.48 35.84 25.73 13.66 4.92 36.26 32.11\nCLaRa-Phi4-mini\n4x 55.53 45.94 55.28 43.24 25.96 15.14 55.57 50.18\n16x 58.62 48.90 56.47 43.45 23.07 12.49 56.85 51.57\n32x 61.15 50.45 56.31 43.13 21.28 11.29 51.21 45.70\nOracle\n64x 57.63 46.62 52.39 38.94 22.38 11.63 48.11 42.83\n128x 56.26 44.77 50.74 37.68 22.27 11.79 47.64 42.31\n256x 54.55 43.09 50.00 36.78 20.92 11.01 46.85 41.66\nTable7 End-to-End QA Performance. * indicates that the results are reported from the DRO paper. CR means\ncompression rate.\n20", "meta": {"page": 20, "source": "2511.18659v2.pdf"}}
{"text": "NQ HotpotQA Musique 2Wiki\nModels CR\nR@1 R@3 R@5 R@1 R@3 R@5 R@1 R@3 R@5 R@1 R@3 R@5\nBM25 1x 6.57 21.89 35.99 25.12 48.86 62.09 13.00 29.12 39.13 15.23 37.06 51.40\nBGE-reranker 1x 11.18 33.56 47.78 28.53 67.91 85.93 18.32 42.45 54.13 22.21 54.95 68.32\nRankZephyr-7B 1x 16.00 40.20 52.40 38.00 63.70 75.90 21.04 40.61 50.65 33.40 61.55 74.69\nSetwise 1x 12.60 38.20 52.10 30.40 60.30 74.60 18.50 40.30 51.60 26.20 57.99 71.61\nRank-R1 1x 8.95 30.60 46.72 24.87 56.56 73.06 16.77 38.49 51.68 24.27 59.02 77.02\nPretraining-initialized\n4x 31.57 62.34 74.96 46.33 71.76 82.84 32.35 50.30 60.02 42.38 74.51 85.02\n16x 30.66 61.05 73.30 44.00 69.66 80.45 29.44 45.07 54.63 40.03 65.48 77.27\n32x 30.19 60.89 73.93 45.69 73.76 83.80 31.34 48.20 57.86 42.08 68.09 79.41\nSup-Mistral-7B\n64x 29.88 60.38 73.53 45.92 74.35 84.46 31.32 46.91 55.93 42.13 68.26 79.52\n128x 29.02 59.25 72.33 44.79 73.76 83.76 29.42 45.97 55.21 40.65 65.61 76.97\n256x 27.54 56.67 70.00 44.50 73.89 83.81 29.13 46.33 56.04 39.99 64.87 76.22\n4x 32.62 63.71 76.38 47.07 90.90 96.21 30.45 59.99 72.46 34.37 68.08 79.13\n16x 28.45 58.97 71.88 42.01 77.80 87.85 22.57 44.34 58.83 32.04 67.12 82.50\n32x 28.06 59.68 73.21 43.84 81.32 90.32 28.22 56.19 70.19 32.36 64.96 81.01\nCLaRa-Mistral-7B\n64x 27.69 59.11 72.79 43.17 79.35 89.54 24.96 49.13 62.37 32.84 63.45 79.40\n128x 28.17 59.64 73.30 40.70 73.60 84.70 20.74 41.29 54.86 31.80 63.41 79.48\n256x 25.33 55.62 69.79 39.68 71.38 83.34 19.58 38.57 51.33 30.81 56.56 72.59\n4x 24.22 52.18 66.15 39.85 66.57 78.94 26.13 42.16 51.27 37.43 62.27 73.49\n16x 28.16 57.82 71.13 42.18 67.66 79.18 28.48 43.75 53.55 39.96 55.62 66.99\n32x 27.66 57.96 71.69 41.66 69.12 80.30 27.87 42.92 51.16 39.11 61.05 71.51\nSup-Phi4-mini\n64x 27.40 57.76 72.01 41.40 70.39 81.34 28.11 44.27 53.46 38.27 61.67 73.13\n128x 27.45 57.08 70.61 41.69 70.47 81.28 27.62 42.85 51.41 37.85 63.81 74.89\n256x 25.67 54.20 67.85 40.82 67.54 78.62 27.03 41.45 49.93 36.78 60.32 71.13\n4x 8.58 28.21 44.61 18.38 42.90 60.01 10.56 26.81 40.08 17.05 38.03 52.63\n16x 20.94 48.89 64.34 23.90 49.17 64.38 15.07 30.79 42.49 21.16 46.83 64.10\n32x 27.63 58.37 72.37 28.64 55.48 70.00 16.43 34.31 46.55 30.30 58.89 75.98\nCLaRa-Phi4-mini\n64x 29.17 60.08 73.83 36.25 65.67 78.44 16.38 33.36 46.23 27.74 53.69 69.63\n128x 29.63 60.54 74.02 37.94 63.96 75.93 17.67 34.90 46.39 30.79 58.53 74.64\n256x 27.50 57.90 71.58 37.10 63.30 75.23 17.67 34.13 44.98 31.79 60.53 76.90\nInstruction-tuned-initialized\n4x 28.33 58.52 71.96 42.40 65.93 77.40 27.87 45.47 55.39 39.90 62.02 74.35\n16x 28.20 57.24 69.57 42.24 67.91 79.33 26.80 42.33 50.68 39.22 59.77 70.38\n32x 27.56 56.70 69.58 44.88 71.02 81.54 29.29 43.86 52.22 42.21 55.47 66.56\nSup-Mistral-7B\n64x 25.70 54.11 66.80 44.94 73.32 84.24 29.02 44.21 53.28 42.09 54.68 66.03\n128x 25.05 53.12 65.73 45.14 74.56 85.18 28.11 43.18 52.40 42.27 58.47 69.94\n256x 25.15 52.61 65.27 44.60 73.95 85.18 28.69 44.32 53.25 41.89 52.64 63.77\n4x 24.66 55.27 69.82 25.73 52.20 68.63 18.06 37.67 50.27 28.51 57.74 73.00\n16x 23.66 52.43 66.79 35.85 67.69 81.13 18.95 37.21 50.71 12.85 32.17 47.90\n32x 21.54 49.50 65.13 35.78 65.94 80.11 16.75 36.93 50.48 17.14 39.59 55.75\nCLaRa-Mistral-7B\n64x 20.77 49.52 64.71 33.65 63.93 78.47 16.13 34.13 46.41 17.40 38.10 51.61\n128x 20.07 47.19 61.98 32.24 61.83 77.36 13.77 29.70 41.93 19.43 42.62 57.31\n256x 19.39 46.91 62.66 29.71 56.66 71.58 15.84 31.92 43.57 20.12 42.26 56.29\n4x 22.38 49.29 62.81 39.91 66.12 77.59 26.11 41.34 50.39 37.79 61.89 72.96\n16x 23.85 52.04 65.97 40.92 65.97 77.75 26.47 42.17 51.07 38.52 53.51 64.79\n32x 23.72 52.09 65.43 41.53 67.81 79.44 27.41 41.40 50.23 39.79 51.25 60.99\nSup-Phi4-mini\n64x 22.34 49.46 63.40 40.93 68.95 80.57 26.17 42.17 51.14 38.47 49.72 58.88\n128x 22.80 50.64 63.30 40.47 67.99 80.36 27.05 41.82 50.21 38.95 50.31 59.78\n256x 22.27 49.31 62.15 39.76 65.61 78.39 26.54 41.06 48.39 37.68 49.09 56.86\n4x 3.98 17.02 31.76 13.19 33.04 48.40 7.60 21.09 31.03 20.40 39.25 52.00\n16x 8.15 25.82 40.84 18.00 41.31 57.10 7.82 18.24 28.19 21.16 40.80 51.96\n32x 16.94 42.64 58.30 31.30 58.02 71.88 13.71 29.99 40.74 29.85 53.24 66.04\nCLaRa-Phi4-mini\n64x 17.63 44.29 59.33 32.09 59.98 74.35 15.93 32.42 44.22 29.77 56.59 71.59\n128x 21.90 50.34 65.14 30.53 55.15 68.30 12.81 27.30 37.66 25.54 47.53 61.28\n256x 19.32 45.89 60.39 31.18 56.05 69.80 12.24 26.19 37.40 27.54 51.71 64.10\nTable8 Retrieval performance (Recall@1/3/5) on the Mistral-7B and Phi-4-mini model across different reranking\nmethods under various compression ratios (CR) and initialization settings on four QA datasets. Sup- denotes models\ntrained with labeled data using contrastive learning for the reranker.\n21", "meta": {"page": 21, "source": "2511.18659v2.pdf"}}
{"text": "B.2 Models\nFor document retrieval, we employ BGE-large-en-v1.51 as the retriever for coarse ranking. Unless otherwise\nspecified, we adopt Mistral-7B-Instruct-v0.22 as the default backbone for all experiments. Additionally,\nwe evaluate the proposed method on Phi-4-mini-instruct3 to assess its generalization across different\nmodel families. On top of the backbone model, we implement three LoRA modules: a compressor, a query\nreasoner, and a generation module.\nB.3 EvaluationMetrics\nFollowing previous studies (Cheng et al., 2025; Louis et al., 2025a), we evaluate the compressor using the\nCover Exact Match (ACC) metric, which measures whether the ground-truth answer is included in the\ngenerated output. For the reranker, we report Recall@k (k ‚àà{1,3,5}), defined as the proportion of positive\ndocuments appearing within the top-k ranked results. For the generation model, we adopt two standard QA\nmetrics: Exact Match (EM) and F1. The EM score measures the percentage of predicted answers that\nexactly match the gold answers, while the F1 score computes the token-level overlap between predictions and\nreferences, reflecting the harmonic mean of precision and recall.\nHyperparameter Value\nLR Scheduler cosine\nOptimizer AdamW\nEpochs 1\nLoRA Layers (r) all-linear\nLoRA Rank (r) 16\nLoRA Dropout 0.1\nLoRA Alpha 32\nLoRA Rank (r) 16\nWarmup Ratio 0.03\nMax Gradient Norm 1.0\nDocuments Max Tokens 256\nCompression learning\nŒª 0.1\nBatch Size 128\nLearning Rate (LR) 1√ó10‚àí4\nEnd-to-end learning\nBatch Size 32\nLearning Rate (LR) 5√ó10‚àí6\nTable11 Hyperparameter settings used in our experiments.\nB.4 ImplementationDetails\nTable 11 summarizes the hyperparameters used for all LoRA modules and training stages. Specifically,\nwe employ separate configurations for the compression learning, and end-to-end training phases. During\nend-to-end learning, both the query reasoner and the generator are initialized from the compressor-trained\ncheckpoints. Following Shi et al. (2025), for each query x, we first retrieve the top-20 documents from the\ncorpus using BGE-large-en-v1.5, obtain their corresponding compressed representations, and then pass\nthem along with the query into the query reasoner to identify the top-k (k =5) ranked documents, which are\nsubsequently fed into the generator.\nFor corpus preprocessing, each document is segmented into chunks of 256 tokens. We extensively evaluate our\nmodel under different compression ratios œÅ‚àà{4,16,32,64,128,256}, where the number of memory tokens\nis computed as 256/œÅ. All experiments are conducted on 8√ó100 H100 GPUs. Unless otherwise stated, all\ntraining runs are performed for a single epoch.\n1https://huggingface.co/BAAI/bge-large-en-v1.5\n2https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2\n3https://huggingface.co/microsoft/Phi-4-mini-instruct\n22", "meta": {"page": 22, "source": "2511.18659v2.pdf"}}
{"text": "B.5 Baselines\nIn this section, we provide detailed descriptions of all baseline methods used for comparison under different\nexperimental settings. We categorize them into three groups: (1) compression baselines, (2) retrieval and\nreranking baselines, and (3) end-to-end QA baselines.\nB.5.1 CompressionBaselines\nAutoCompressor.(Chevalieretal.,2023) This method segments a long document into chunks, appends a <Sum>\ntoken at the end of each chunk, and trains the model to produce a fixed number of summary vectors. During\ntraining, the model is fine-tuned with a standard language modeling cross-entropy loss, with a stop-gradient\nappliedtopastsummaryvectors. Atinferencetime,themodelfirstcompressesandthenreusesthesummaries,\nachieving efficient long-context reasoning at significantly reduced cost.\nXRAG.(Chengetal.,2025) XRAG treats retrieved document embeddings as an additional retrieval modality,\nmapping them into the language model‚Äôs representation space via a lightweight projection layer. This enables\nretrieval-augmented generation with as few as a single ‚Äúdocument token.‚Äù XRAG adopts a two-stage training\nstrategy: (1) Paraphrase Pretraining to align document embeddings with textual semantics, and (2) Context-\nAware Instruction Tuning with self-distillation to optimize retrieval utilization. Only the projection layer is\ntrained, while both the retriever and language model remain frozen, achieving compression ratios up to 178√ó.\nCOCOM.(Rauetal.,2025) COCOM maps each retrieved document into a compact sequence of context\nembeddings (e.g., compressing hundreds of tokens into 4‚Äì128 embeddings), which reduces input length and\nacceleratesgeneration. Itjointlytrainsacompressorandageneratorwithtwoobjectives: (i)anauto-encoding\nreconstruction loss to preserve semantic information, and (ii) a conditional generation loss to ensure high-\nquality answers from compressed contexts. The framework also supports multi-document compression and\ncross-document fusion, and offers a lightweight variant (COCOM-light) using BERT as the compressor.\nPCC consists of an encoder and a transformer-based converter. The encoder extracts\nPCC.(Daietal.,2025)\ncompact semantic representations, while the converter adjusts their dimensionality and semantics through\ntwo MLP layers so that the compressed memory can be directly fed into any LLM. The model is pretrained\n(with the LLM frozen) using a combination of auto-encoding reconstruction and auto-regressive completion\ntasks to retain generation-relevant information. Domain-specific fine-tuning is then performed on limited data\nfor RAG QA, ICL reasoning, and dialogue tasks.\nLLMLingua-2 constructs a large-scale extractive compression dataset using\nLLMLingua-2.(Panetal.,2024)\nGPT-4-generated high-fidelity summaries. It formulates compression as a token-level binary classification\nproblem (keep or remove), where a bidirectional Transformer encoder (e.g., XLM-RoBERTa) estimates the\nretention probability of each token. Tokens are ranked by their probabilities to achieve 2‚Äì5√ó compression\nwhile maintaining semantic completeness.\nPISCO.(Louisetal.,2025a) PISCO introduces trainable memory tokens appended to the document, jointly\nfine-tuned with LoRA adapters to compress text by up to 1/16 of its original length. It employs sequence-\nlevel knowledge distillation (SKD) from teacher-generated answer sequences to ensure consistency between\ncompressed and uncompressed outputs.\nB.5.2 RetrievalandRerankingBaselines\nA classical lexical retrieval method that scores each document based on term frequency, inverse\nBM25.\ndocument frequency, and document length normalization.\nA recent large-scale, general-purpose reranker that directly predicts the\nBGE-Reranker.(Chenetal.,2023)\nrelevance score between a query and each candidate document, used to reorder initial retrieval results.\n23", "meta": {"page": 23, "source": "2511.18659v2.pdf"}}
{"text": "A7B-parameteropen-sourcererankerdistilledintwostagesfromRankGPT-\nRankZephyr.(Pradeepetal.,2023)\n3.5 and RankGPT-4. It integrates variable-window training, input order shuffling, and teacher-guided ranking\ndata, achieving robust performance under varying document counts and ranking conditions. During inference,\nRankZephyr performs iterative sliding-window ranking using prompt-decoder style generation.\nUnlike pairwise reranking, Setwise compares multiple candidate documents\nSetwise.(Zhuangetal.,2024)\nin a single inference step, greatly reducing LLM calls and prompt length. It leverages classical sorting\nalgorithms(e.g.,heaporbubblesort)anddirectlyestimatesrelevanceprobabilitiesfrommodellogits,avoiding\nstep-by-step list generation.\nA reinforcement learning-based reranking framework that enhances LLM\nRank-R1. (Zhuangetal.,2025)\nreasoning capabilities for document ranking. Built upon the Setwise ranking paradigm, it introduces explicit\nreasoninginstructionsbeforeanswergenerationandoptimizesthemodelviaGroupRelativePolicyOptimization\n(GRPO). The model is trained only with queries and relevance labels, and receives reward signals based on\nprediction correctness and format compliance.\nB.5.3 End-to-EndQABaselines\nThis method decomposes complex questions into sub-questions using the model‚Äôs\nGenGround.(Shietal.,2024)\ninternal knowledge, then refines preliminary answers via retrieved documents for evidence grounding. It\nfurther introduces Instructional Grounding Distillation (IGD), which distills grounding trajectories from\nChatGPT into smaller open models such as Mistral-7B.\nIn-ContextRAG.\nSelects the top-k retrieved documents using the BGE Reranker and feeds them as context to\nthe LLM for direct answer generation.\nReComp retrieves relevant documents and compresses them into concise, query-\nReComp.(Xuetal.,2024)\nrelated summaries via either an extractive or a generative compressor. These summaries are then used as\ncontext for answer generation. Training jointly optimizes both retriever and compressor, allowing selective\nretrieval when documents are unhelpful.\nThis method introduces preference-aligned retrieval and generation. It first\nDPA-RAG.(Dongetal.,2025)\nconstructs preference data by analyzing LLM responses under various retrievals and then aligns both reranker\nand generator through a hybrid of point-wise, pair-wise, and contrastive training objectives.\nImproves robustness of RAG systems through two mechanisms: (i) using an\nRetRobust.(Yoranetal.,2024)\nNLI model to filter irrelevant retrieved texts, and (ii) fine-tuning with mixed relevant/irrelevant retrieval\nsamples so that the model learns when to utilize or ignore retrieval information.\nA context-augmented instruction-tuned model that integrates multi-source conversa-\nChatQA.(Liuetal.,2025)\ntional and instruction data to enhance reasoning and refusal capabilities. It also fine-tunes a dense retriever\non multi-turn QA data, replacing traditional query rewriting modules.\nSelf-RAG.(Asaietal.,2024) Incorporates reflection tokens (e.g., ‚Äúneed retrieval?‚Äù, ‚Äúretrieved relevant?‚Äù, ‚Äúsup-\nported answer?‚Äù) so the model can self-assess and adaptively decide when to retrieve external knowledge.\nTraining combines GPT-4‚Äìgenerated annotated data with self-reflective labeling to enable dynamic retrieval\nand self-critique during inference.\nRAG-DDR.(Lietal.,2025)\nEmploysDifferentiableDataRewards(DDR)toachievefullyend-to-endoptimization\nof RAG systems. It uses rollout-based system rewards and aligns retrieval and generation through Direct\nPreference Optimization (DPO).\n24", "meta": {"page": 24, "source": "2511.18659v2.pdf"}}
{"text": "Models document ordering as a latent variable and alternates between inference and\nDRO.(Shietal.,2025)\noptimization using a variational EM framework. The E-step estimates document order distributions via\nimportancesampling,whiletheM-stepjointlyupdatestheselectorandgeneratorbasedonweightedlikelihood\nmaximization.\nC PretrainingDataQuality\nTo ensure the quality of the constructed pretraining data, we conducted a manual evaluation. We randomly\nsampled 200 examples for each output type, resulting in a total of 600 samples, which were independently\nassessed by one of the authors. The evaluation results indicate that, thanks to our rigorous filtering process,\nalmost all generated samples successfully cover the key information contained in the source documents. Only\n21 instances exhibited mild hallucinations, where the model introduced information not present in the original\ntext. This demonstrates that the synthesized data are of high factual and semantic quality, providing a\nreliable foundation for compression pretraining.\nModels Corpussize NQ HotpotQA Musique 2Wikiqa Average\nNormal\n0.5M 53.38 41.40 10.30 46.67 37.94\nMistral-7B 1M 54.82 43.71 10.63 46.90 39.02\n2M 54.64 43.52 10.55 46.58 38.82\n0.5M 48.82 38.53 7.78 43.57 34.67\nPhi4-mini 1M 48.40 38.47 7.73 43.82 34.61\n2M 49.30 38.62 7.70 43.71 34.83\nOracle\n0.5M 70.33 62.47 29.16 57.97 54.98\nMistral-7B 1M 74.08 68.88 38.97 63.91 61.46\n2M 73.77 69.51 38.31 64.54 61.53\n0.5M 68.31 64.41 29.25 58.22 55.05\nPhi4-mini 1M 69.41 64.42 31.32 58.00 55.79\n2M 69.90 65.32 31.77 58.52 56.38\nTable12 Instruction tuning performance of Mistral-7B and Phi4-mini models under different pretraining corpus\nsizes (0.5M, 1M, 2M). Results are reported on four QA datasets under both Normal and Oracle retrieval settings with\na fixed compression ratio (CR = 32).\nD PretrainingDataScaling\nTo investigate how the number of pretraining samples affects the performance of the compressor, we train\nmodels with varying amounts of pretraining data and assess their performance after instruction tuning on\nfour QA datasets. The results are illustrated in Table 12. We observe that enlarging the pretraining corpus\ngenerally leads to consistent performance improvements across all datasets and both retrieval settings. For\ninstance, under the Normal setting, the Mistral-7B model improves its average score from 37.94 to 39.02 as\nthe corpus size increases from 0.5M to 1M, while the performance remains stable when further scaled to 2M.\nA similar trend can be observed in the Oracle setting, where the model achieves an average gain of over 6\npoints when moving from 0.5M to 1M, indicating that additional pretraining data enhances the compressor‚Äôs\nability to preserve more task-relevant information.\nFor the smaller Phi4-mini model, the improvements are relatively modest, suggesting that model capacity\nmay constrain the benefits of scaling pretraining data. Overall, these findings demonstrate that moderate\nexpansion of pretraining data contributes positively to downstream QA performance, while extremely large\npretraining sets bring diminishing returns.\nE TrainingCurves\nFigures5presentthevalidationlosscurvesduringthecompressionpretrainingstageacrossdifferentcompression\nratios.\nA clear trend emerges: as the compression ratio increases, the validation loss rises for both models. This\neffect is more pronounced for Phi4-mini, where losses at ratios of 128 and 256 diverge sharply. In contrast,\n25", "meta": {"page": 25, "source": "2511.18659v2.pdf"}}
{"text": "Figure5 Validationlosscurvesduringthecompressionpretrainingstageunderdifferentcompressionratios(CR)onthe\nPhi-4-mini (left) and Mistral-7B (right) models.\nFigure6 Validationtrendsofrecallandevaluationlossduringtheend-to-endtrainingstageunderdifferentcompression\nratios (CR) on the NQ (top) and Musique (bottom) datasets.\nMistral-7Bexhibitsrelativelyuniformlossgapsacrosscompressionratios. Wehypothesizethatthisdifference\narises because of capacity. Phi4-mini, with fewer parameters, has limited representational ability. At very\nhigh compression levels (e.g., CR=128), excessive information loss leads to semantic degradation and a steep\nrise in validation loss.\nFigure 6 presents the validation curves during end-to-end training on the NQ and Musique datasets. Recall\nscoresconsistentlyincreasewhileevaluationlossessteadilydecrease,indicatingstableandeffectiveoptimization.\nHigher compression ratios generally yield lower recall and higher loss, mirroring the trends observed during\nthe compression pretraining stage.\nF MoreAnalysis\nF.1 EffectofFreezingtheCompressorandQueryReasoner\nWe investigate the effect of limiting the fine-tuning scope to the generator module while freezing both the\ncompressor and query reasoner. Specifically, we examine two representative compression settings, CR=32 and\nCR=128, and compare model performance when only the generator is fine-tuned during both the instruction\ntuning and end-to-end QA training stages. The results are shown in Table 13 and 14. During the compression\nlearning and instruction tuning stages, we observe that fine-tuning the compressor alongside the generator\nbrings only marginal improvements. For example, under the Normal setting, the average gain of full fine-\ntuning over generator-only tuning is less than 2.0% across most datasets. Considering that in Section 4.4,\nthe instruction-tuned compressor tends to degrade retrieval performance due to its focus on answer-centric\nrepresentations, a promising future direction is to explore how to effectively extract task-relevant information\nfrom compressed representations without directly fine-tuning the compressor itself. In contrast, during the\n26", "meta": {"page": 26, "source": "2511.18659v2.pdf"}}
{"text": "end-to-end learning stage, fine-tuning the query reasoner proves to be more beneficial. A trainable retrieval\nmodule enables the model to identify more relevant documents and provide stronger contextual grounding\nfor the generator. For instance, under the Oracle setting with a compression ratio of 32, the F1 score of\nMistral-7Bimprovesfrom52.54%to70.91%whenjointlyfine-tuningboththequeryreasonerandgenerator.\nThis highlights the crucial role of query reasoner in enhancing overall QA performance within our unified\ntraining framework.\nModels CR NQ HotpotQA Musique 2Wiki Average\nNormal\nGenerator-only\nMistral 32x 52.26 42.66 10.43 45.79 37.78\nMistral 128x 50.72 40.10 9.14 45.52 36.37\nPhi 32x 45.91 37.70 6.95 42.97 33.39\nPhi 128x 38.83 32.30 6.50 42.53 30.04\nFullfinetune\nMistral 32x 54.64 43.52 10.55 46.58 38.82\nMistral 128x 53.36 41.37 10.26 46.40 37.85\nPhi 32x 49.30 38.62 7.70 43.71 34.83\nPhi 128x 43.09 33.92 6.87 43.70 31.90\nOracle\nGenerator-only\nmistral 32x 72.78 67.48 34.38 60.89 58.88\nmistral 128x 66.93 59.66 25.94 58.19 52.68\nphi 32x 65.65 62.76 27.60 56.46 53.12\nphi 128x 52.87 47.51 17.38 48.98 41.68\nFullfinetune\nmistral 32x 73.77 69.51 38.31 64.54 61.53\nmistral 128x 69.96 62.09 30.86 59.08 55.50\nphi 32x 69.90 65.32 31.77 58.52 56.38\nphi 128x 60.44 51.52 19.28 50.29 45.38\nTable13 Instruction tuning results of Mistral and Phi models under different fine-tuning scopes (generator-only vs.\nfinetune-both), retrieval modes (Normal vs. Oracle), and compression ratios (CR = 32, 128) on four QA datasets.\nHotpotQA 2Wiki\nModels CR\nF1 EM F1 EM\nNormal\nGenerator-only\nMistral 32x 38.40 28.24 39.93 35.80\nMistral 128x 38.26 28.26 41.34 37.29\nPhi4 32x 32.91 23.51 35.99 32.14\nPhi4 128x 31.42 21.89 35.54 31.32\nFullfinetune\nMistral 32x 41.84 31.26 43.23 38.98\nMistral 128x 42.26 31.78 41.80 37.37\nPhi4 32x 37.14 26.99 38.15 33.82\nPhi4 128x 34.73 24.95 36.41 32.23\nOracle\nGenerator-only\nMistral 32x 52.54 40.11 45.43 40.98\nMistral 128x 51.60 39.06 44.64 40.22\nPhi4 32x 45.91 34.20 40.33 36.12\nPhi4 128x 39.46 28.06 37.05 32.57\nFullfinetune\nMistral 32x 70.91 57.07 66.32 61.12\nMistral 128x 66.51 52.30 64.82 58.97\nPhi4 32x 51.06 38.33 50.68 45.41\nPhi4 128x 52.68 38.49 49.97 44.11\nTable14 End-to-end QA performance of Mistral and Phi models under different fine-tuning scopes (generator-only\nvs. finetune-both), retrieval modes (Normal vs. Gold), and compression ratios (CR = 32, 128) on HotpotQA and\n2WikiQA datasets.\nF.2 Retrievalnumbergeneralization\nWe further explore the impact of varying the number of retrieved documents (top-k) during testing in our\nend-to-end training framework. During training, the model is consistently trained with the top-5 retrieved\ndocuments, while at test time, we vary k from 1 to 10 to examine the model‚Äôs sensitivity to retrieval size.\nThe results are presented in Figure 7. As shown in the figure, the F1 score generally exhibits a rapid\n27", "meta": {"page": 27, "source": "2511.18659v2.pdf"}}
{"text": "Figure7 Performance of varying the number of retrieved documents (k) during testing on different QA datasets.\nincrease followed by a gradual decline as k increases. However, the performance drop remains relatively small,\nindicating that our trained query reasoner and generator demonstrate good generalization with respect to the\nnumber of retrieved documents during inference.\nF.3 EffectofQueryReasonerInitialization\nWe evaluate the effect of initializing the query reasoner with the pretrained compressor parameters versus\nrandom initialization on the HotpotQA and 2Wiki datasets, as shown in Table 15. The results demonstrate\nthat compressor-initialized models consistently outperform their randomly initialized counterparts across all\nsettings. This performance gain (e.g., from 66.84%‚Üí70.91% F1 and 62.68%‚Üí66.32% F1 on HotpotQA and\n2Wiki, respectively) indicates that the pretrained compressor provides a strong prior for learning effective\nquery reasoning representations, as it already encodes semantic relationships between queries and document\ncontent during the compression pretraining stage.\nHotpotQA 2Wiki\nModel CR Retrieval Mode\nF1 EM F1 EM\nMistral-7B 32x Normal 39.48 29.12 39.90 35.79\nw/CompressorInit. 32x Normal 41.84 31.26 43.23 38.98\nMistral-7B 32x Oracle 66.84 52.91 62.68 57.55\nw/CompressorInit. 32x Oracle 70.91 57.07 66.32 61.12\nMistral-7B 128x Normal 37.25 27.38 38.55 34.69\nw/CompressorInit. 128x Normal 42.26 31.78 41.80 37.37\nMistral-7B 128x Oracle 62.06 48.37 60.63 54.87\nw/CompressorInit. 128x Oracle 66.51 52.30 64.82 58.97\nTable15 End-to-End QA Performance with Randomly Initialized vs. Compressor-Initialized Query Reasoner\nF.4 EfficiencyAnalysis\nWe evaluate the inference efficiency of our framework under different compression ratios. Specifically, for\neach query, we retrieve 20 candidate documents, compress them into 5 document representations using the\ncompressor, and then generate the final answer based on these 5 compressed representations and the query.\nThe average inference time for each stage is reported in Table 16. All timing statistics are measured on a\nsingle NVIDIA H100 GPU with 80GB memory.\nAs shown in the results, decoding with compressed representations takes only about 40% of the time required\nwhen using full-text documents. Although compressing 20 documents is relatively time-consuming, this step\ncan be performed offline; hence, it does not affect real-time inference latency during query answering. This\nmakes the overall computational cost acceptable for practical deployment. We also observe that for the\nMistral model, compression time tends to decrease as the compression ratio increases, while both decoding\nand query retrieval times remain relatively stable across different compression settings.\n28", "meta": {"page": 28, "source": "2511.18659v2.pdf"}}
{"text": "Models CR Compression Time Query Time Decoding Time\nMistral-7B Puretext ‚Äì ‚Äì 1290.57\nMistral-7B 4x 1092.29 99.69 532.73\nMistral-7B 16x 922.85 94.17 502.78\nMistral-7B 32x 904.22 92.16 514.75\nMistral-7B 64x 893.76 95.14 521.09\nMistral-7B 128x 876.99 95.24 518.41\nMistral-7B 256x 835.87 90.76 521.03\nPhi4-mini Puretext ‚Äì ‚Äì 870.29\nPhi4-mini 4x 674.78 94.34 342.05\nPhi4-mini 16x 574.46 89.53 343.01\nPhi4-mini 32x 561.17 84.35 358.04\nPhi4-mini 64x 604.89 85.33 354.77\nPhi4-mini 128x 594.55 91.73 360.23\nPhi4-mini 256x 789.49 99.47 354.87\nTable16 Average inference time (in milliseconds) for compression, retrieval, and decoding across different compression\nratios (CR) on Mistral-7B and Phi4-mini models.\nG FidelityandGroundingAnalysis\nIn this section, we aim to understand how much essential information is retained in our compressed represen-\ntations, and to what extent the generated answers remain grounded to the input documents and queries after\nboth compression learning and end-to-end training.\nG.1 InformationPreservation\nDuring compression representation pretraining, we include a paraphrasing objective that allows the generation\nmodel to reconstruct the original text from the compressed representation. We consider two evaluation\nsettings: (1) unseen data, consisting of positive documents of downstream QA tasks that were not used in\npretraining, and (2) seen data, where we randomly sample 4,000 documents from the pretraining corpus.\nWe evaluate the reconstruction quality using several metrics: BERTScore (Zhang* et al., 2020) (which\nmeasures semantic similarity between texts), ROUGE-1 and ROUGE-L (which capture lexical overlap), and\nfollowing ≈Åajewska et al. (2025), we also compute the entity preservation ratio, which measures the proportion\nof entities from the input text that are preserved in the reconstructed text4.\nThe results are shown in Table 17. We observe that our model achieves a high BERTScore of nearly 90%,\nwhich remains stable across different compression ratios. This indicates that the compressed representations\nsuccessfully retain most of the semantic information from the original text. For ROUGE-1, ROUGE-L, and\nentity preservation, the model also maintains relatively high scores‚Äîover 50% on average. We further observe\nthat as the compression ratio increases, the lexical overlap and entity preservation metrics gradually decline,\nsuggesting that fewer memory tokens make it harder to reconstruct the exact surface form of the original\ntext. However, the consistently high semantic similarity scores imply that the key meaning is preserved. This\nphenomenon may indicate that when using fewer memory tokens, the model tends to generate paraphrased\nexpressions to maintain the original semantics. We leave further exploration of this linguistic compression\nbehavior for future work.\n4EntityextractionisperformedusingtheSpaCylibrary.\n29", "meta": {"page": 29, "source": "2511.18659v2.pdf"}}
{"text": "SeenData UnseenData\nModels CR\nBERT R-1 R-L Entity BERT R-1 R-L Entity\n4x 90.67 55.88 40.12 54.78 91.45 59.74 44.09 60.04\n16x 90.63 56.12 40.33 54.78 91.43 59.97 44.10 59.88\n32x 90.56 56.21 40.10 53.91 91.39 60.28 43.98 59.33\nMistral-7B\n64x 90.28 55.54 38.86 51.45 91.24 60.09 43.42 58.26\n128x 89.84 54.12 36.56 47.75 91.00 59.61 42.48 55.75\n256x 89.19 51.75 33.12 42.12 90.51 57.89 39.59 52.38\n4x 90.93 58.48 42.16 57.86 91.70 62.00 45.10 63.14\n16x 90.77 58.20 41.49 56.28 91.66 62.20 45.31 62.22\n32x 90.36 57.04 39.40 52.38 91.42 61.71 44.34 59.64\nPhi4-mini\n64x 89.53 54.27 35.40 45.28 90.84 60.20 41.72 54.47\n128x 88.26 49.30 29.65 34.98 89.61 55.68 35.58 43.89\n256x 88.13 49.05 29.10 34.27 89.52 55.27 35.22 43.61\nTable17 Evaluationofinformationpreservationunderdifferentcompressionratios(CR)onseenandunseendocuments\nusing BERTScore, ROUGE, and entity preservation.\nNQ HotpotQA Musique 2Wiki Average\nModels CR Retrieval\nFaith Fc Faith Fc Faith Fc Faith Fc Faith Fc\nPretraining-initialized\n4x 81.57 8.39 67.42 11.80 55.80 9.57 56.45 5.15 65.31 8.73\n16x 75.85 8.97 62.50 13.58 49.64 10.34 51.02 5.04 59.75 9.48\n32x 72.65 7.34 61.40 11.86 51.35 8.18 53.30 7.30 59.67 8.67\nNormal\n64x 67.49 8.29 57.95 10.85 46.55 10.00 44.44 5.16 54.11 8.57\n128x 65.99 8.67 56.50 10.35 44.52 9.50 43.26 4.54 52.57 8.27\n256x 64.74 7.42 53.68 12.34 40.79 7.87 40.88 4.85 50.02 8.12\nMistral-7B\n4x 86.73 9.42 83.75 18.72 67.67 13.60 80.16 6.26 79.58 12.00\n16x 81.13 9.74 83.16 19.19 63.89 11.05 73.96 4.12 75.54 11.03\n32x 79.34 8.16 81.87 16.31 65.27 10.79 71.60 3.71 74.52 9.74\nOracle\n64x 74.43 8.07 78.39 16.63 55.89 10.09 67.61 4.30 69.08 9.77\n128x 74.63 10.40 77.17 12.60 55.14 10.47 61.62 4.00 67.14 9.37\n256x 71.13 9.63 73.69 16.49 51.35 10.22 56.03 4.67 63.05 10.25\n4x 79.45 7.47 62.46 10.06 50.57 11.13 51.05 5.27 54.69 8.48\n16x 74.99 8.13 61.45 12.84 51.61 10.04 48.59 5.17 59.16 9.05\n32x 66.58 5.74 59.22 11.64 45.44 9.03 45.16 3.69 54.10 7.52\nNormal\n64x 59.66 8.36 50.44 9.42 39.52 8.53 39.79 4.13 47.35 7.61\n128x 60.26 8.18 51.83 10.57 35.08 9.47 34.72 4.61 45.48 8.20\n256x 57.65 7.78 45.57 8.71 33.79 7.38 32.73 5.71 42.44 7.39\nPhi4-mini\n4x 82.51 8.32 82.88 18.79 60.67 11.02 76.81 4.19 75.72 10.58\n16x 81.00 8.81 81.73 17.55 63.38 10.54 69.61 3.03 73.93 9.98\n32x 75.42 10.20 74.79 16.72 54.49 10.30 61.67 3.26 66.59 10.12\nOracle\n64x 68.72 7.66 71.63 15.60 49.55 7.51 56.36 2.63 61.56 8.35\n128x 63.49 8.99 66.46 14.84 44.17 8.38 48.50 2.07 55.65 8.57\n256x 61.36 9.80 64.48 11.60 44.01 10.76 53.23 3.10 55.77 8.82\nInstruction-tuned-initialized\n4x 54.67 39.69 30.21 36.15 12.68 11.08 5.50 38.97 25.76 31.47\n16x 52.13 39.13 37.68 39.91 13.12 12.55 11.13 36.17 28.52 31.94\n32x 49.33 37.64 36.08 36.97 12.25 10.29 12.93 40.87 27.65 31.44\nNormal\n64x 50.60 36.92 35.35 36.71 11.84 10.52 13.50 34.24 27.82 29.60\n128x 49.03 36.40 36.52 39.58 11.77 10.56 12.13 35.57 27.36 30.53\n256x 50.90 40.02 34.95 38.75 11.81 11.41 15.73 37.67 28.35 31.96\nMistral-7B\n4x 69.55 71.58 67.94 68.23 23.86 37.92 38.43 64.61 49.95 60.59\n16x 69.57 67.79 60.25 60.83 18.85 27.60 39.90 60.19 47.14 54.10\n32x 65.97 63.60 59.77 61.72 21.91 30.38 33.40 56.57 45.26 53.07\nOracle\n64x 63.90 62.81 57.47 56.46 18.60 21.99 27.40 52.27 41.84 48.38\n128x 68.57 60.98 55.75 57.86 16.19 21.43 29.40 55.38 42.48 48.91\n256x 65.60 63.81 55.46 56.30 16.80 20.74 28.40 52.97 41.57 48.46\n4x 49.50 41.02 28.13 35.57 12.65 13.56 6.37 34.37 24.16 31.13\n16x 41.57 30.75 27.70 32.59 9.82 8.21 7.87 35.37 21.74 26.73\n32x 41.07 26.85 28.64 30.10 8.58 8.97 10.63 31.36 22.23 24.32\nNormal\n64x 35.98 25.25 28.57 29.71 11.37 11.03 10.13 34.87 21.51 25.21\n128x 38.47 26.75 30.23 29.52 8.06 8.25 11.03 31.00 21.95 23.88\n256x 37.50 27.89 26.48 25.10 9.34 9.03 10.80 31.74 21.03 23.44\nPhi4-mini\n4x 65.13 59.84 43.67 52.32 14.57 22.90 18.37 48.38 35.43 45.86\n16x 66.45 62.90 46.57 52.31 13.28 18.58 21.87 47.04 37.04 45.21\n32x 60.20 53.13 46.27 45.89 13.70 17.42 25.30 43.71 36.37 40.04\nOracle\n64x 57.35 52.66 47.40 48.79 13.18 18.28 18.63 43.99 41.13 40.93\n128x 55.30 51.05 42.34 42.43 12.80 17.51 18.67 44.84 32.28 38.96\n256x 52.80 47.28 45.35 46.32 11.98 14.93 23.50 40.64 33.41 37.29\nTable18 Groundingevaluationof Mistral-7BandPhi4-minimodelsunderdifferentinitializationsettings(pretraining-\ninitializedvs. instruction-tuned-initialized),retrievalmodes(Normalvs. Oracle),andcompressionratios(CR).Metrics\ninclude Faithfulness (Faith) and Factual Correctness (Fc) across four QA datasets.\n30", "meta": {"page": 30, "source": "2511.18659v2.pdf"}}
{"text": "G.2 GroundingAnalysis\nWe further evaluate the grounding quality between the generated answers and the compressed document\nrepresentations under both compression evaluation and end-to-end evaluation settings. We adopt the\nRAGAs (Es et al., 2025) package, which implements the LLM-as-a-Judge paradigm for assessing generation\nquality. Two key metrics are used: faithfulness, which measures whether the generated answer is faithful to\nthe provided context and relevant to the query, and factual correctness, which evaluates whether the answer is\nfactually supported by the context. We employ GPT-4o-mini as the judging model.\nThe results are presented in Table 18. For the compression evaluation, our model achieves consistently high\nfaithfulness scores, particularly when positive documents are included, indicating that the model generates\nanswers more closely aligned with the query. However, the factual correctness scores are comparatively\nlower, consistent with findings reported in ≈Åajewska et al. (2025). We hypothesize that this is because, after\ninstruction tuning, the generation model tends to produce longer and more elaborative answers, occasionally\nintroducingtokensthatdonotappearintheoriginalcontext. Wealsoobserveadecreasingtrendinfaithfulness\nas the compression ratio increases and model size decreases.\nFor the end-to-end evaluation, the model demonstrates strong performance across both metrics. In particular,\nunder the Mistral-7B model with a compression ratio of 4 and the top-20 retrieval setting containing positive\ndocuments, faithfulness and factual correctness reach 49.95 and 60.59, respectively. The higher factual\ncorrectness is likely due to the use of short gold answers during training, which encourages the model to\ndirectly copy relevant words or phrases from the retrieved documents. Other observed trends are consistent\nwith those in the compression evaluation results.\nH ParaphraseCaseStudy\nTo better understand the nature of information captured by our compressed representations and the query\nreasoner, we analyze how effectively the compressed representations preserve key semantic information. As\nillustratedinTable19,thegenerationsconditionedoncompressedrepresentationslargelypreservetheessential\nsemantic content of the original documents, while substantially altering surface expressions such as sentence\nstructure and word order. This observation is consistent with our pretraining objective, which encourages\nthe model to encode core semantic meaning rather than memorize lexical sequences. The model‚Äôs ability to\nreconstructparaphrasedvariantsoftheoriginaltextdemonstratesthatthelearnedcompressedrepresentations\nsuccessfully capture high-level semantic knowledge in a compact form.\n(a) Without MSE (b) With MSE\nFigure8 t-SNEvisualizationofdocumentrepresentationsandcompressed(memorytoken)representationsof Mistral-\n7B under the compression ratio of 32.\n31", "meta": {"page": 31, "source": "2511.18659v2.pdf"}}
{"text": "Examples of Original and Paraphrased Documents Generated from Compressed Represen-\ntations\nExample 1\nOriginal Document: This a document about SMK Bukit Kepayang. 1) Tuan Haji Mat Rohani Bin Kamar\nwas the first Principal of the School. (2004 - 2008); 2) Madam Hajah Sadariah Binti Haji Md. Jali is the current\nPrincipaloftheschool. (2008-2014). MadamHajahSadariahachievedthetitleMasterPrincipleintheyear2011\ndue to outstanding performance and achievements in that year.\nParaphrased Document: From 2004 to 2008, Tuan Haji Mat Rohani Bin Kamar served as the Princi-\npal of SMK Bukit Kepayang. He was succeeded by Madam Hajah Sadariah Binti Haji Md. Jali, who led\ntheschooluntil2014. In2011,shewashonoredwiththetitleofMasterPrincipal forherexceptionalachievements.\nExample 2\nOriginal Document: This is a document about Sikorsky SH-60 Seahawk and the U.S. 5th Fleet. The ALMDS\nis a sensor system designed to detect, classify, and localize floating and near-surface moored mines in littoral\nzones, straits, and choke points. The system is operated from an MH-60S, which gives it a countermine role\ntraditionally handled by the MH-53E Sea Dragon, allowing smaller ships the MH-53E can‚Äôt operate from to be\nused in the role. The ALMDS beams a laser into the water to pick up reflections, then produces a video im-\nageforgroundpersonneltodetermineiftheobjectisamine. TheMH-60SwillutilizetheBAESystemsArcherfish.\nParaphrased Document: TheALMDSsystem,designedtodetectandclassifyfloatingandnear-surfacemines\nin strategic locations such as straits, littoral zones, and choke points, is operated from an MH-60S helicopter.\nThis platform enables the U.S. Navy‚Äôs 5th Fleet to perform a countermine mission that was previously the\nresponsibility of the MH-53E Sea Dragon. The MH-60S is equipped with the BAE Systems Archerfish system,\nwhich plays a key role in the process. The ALMDS emits a laser pulse into the water, which reflects.\nExample 3\nOriginal Document: This a document about Pinewild Women‚Äôs Championship. The Pinewild Women‚Äôs\nChampionship was a golf tournament on the LPGA Tour, played only in 1995. It was played at the Pinewild\nCountry Club of Pinehurst in Pinehurst, North Carolina. Rosie Jones was the winner, beating Dottie Pepper on\nthe first hole of a sudden-death playoff.\nParaphrased Document: In 1995, the Pinewild Women‚Äôs Championship took place as a single-year event on\nthe LPGA Tour. The competition was held at the Pinewild Country Club of Pinehurst, located in Pinehurst,\nNorth Carolina. Rosie Jones emerged victorious, securing the title by defeating Dottie Pepper in a sudden-death\nplayoff on the first hole.\nTable19 Examples of Original and Paraphrased Documents generated from compressed representations.\nI Prompts\nFigures 9‚Äì16 illustrate the prompts used during the data synthesis process. Specifically, we employ different\nprompting strategies for (1) generating QA pairs, (2) producing paraphrased documents, (3) validating\ninformation completeness, and (4) completing missing information. Additionally, Figure 17 shows the prompt\ntemplateusedbythegenerationmodeltoanswerquestionsbasedonthecompresseddocumentrepresentations.\n32", "meta": {"page": 32, "source": "2511.18659v2.pdf"}}
{"text": "Prompt for Simple Question Generation\nYou are given a document delimited by <doc> and </doc>. Your job is to read the given\ndocument and generate a comprehensive set of multi-hop questions that fully cover all\nthe key information in the text.\n<doc>\n<INSERT DOCUMENT HERE>\n</doc>\nQuestion Requirements:\nYou should generate as many questions as necessary to fully cover all the key facts in\nthe document.\n(1) Each question must be self-contained, meaning it should be understood by the user\nwithout seeing the document.\n(2) Each question must cover only one or at most two distinct key pieces of information.\n(3) The questions must be non-overlapping ‚Äî no two questions should target the same\npiece of information.\n(4) The questions should be simple factual recall only ‚Äî do not require inference,\nreasoning, or summarization.\n(5) Your output should be a list of self-contained, non-overlapping factual questions\nthat together comprehensively cover all the key information in the document.\nThere are some examples:\n{3 demonstrations}\nYour output should be a JSON object with the following format:\n{\n\"Question1\": \"...\",\n\"Question2\": \"...\",\n...,\n\"QuestionN\": \"...\"\n}\nFigure9 Prompt used for simple question generation.\n33", "meta": {"page": 33, "source": "2511.18659v2.pdf"}}
{"text": "Models Datacomposition NQ HotpotQA Musique 2Wikiqa Average\nNormal\nNo-pretrain 53.03 40.63 9.68 46.64 37.50\nSimpleQA 53.84+0.81 42.20+1.57 10.26+0.58 46.68+0.04 38.25+0.75\nMistral-7B Para 54.52+1.49 43.05+2.42 10.51+0.83 46.41-0.23 38.62+1.12\nSimpleQA+ComplexQA 55.48+2.45 43.00+2.37 10.67+0.99 46.39-0.25 38.88+1.38\nSimpleQA+ComplexQA+Para 54.64+1.61 43.52+2.89 10.55+0.87 46.58-0.06 38.82+1.32\nNo-pretrain 48.10 37.65 7.61 44.68 34.51\nSimpleQA 48.56+0.46 38.91+1.26 8.19+0.58 43.70-0.98 34.84+0.33\nPhi4-mini Para 48.65+0.55 38.41+0.76 7.74+0.13 44.11-0.57 34.73+0.22\nSimpleQA+ComplexQA 49.47+1.37 38.88+1.23 8.03+0.42 43.96-0.72 35.08+0.57\nSimpleQA+ComplexQA+Para 49.30+1.20 38.62+0.97 7.70+0.09 43.71-0.97 34.83+0.32\nTable20 Effect of pretraining data composition on instruction-tuning performance under Normal (top-5 retrieval)\nsettings under the 32 compression ratio. We report the absolute score change (¬±) for each pretraining data setting\nrelative to the No-pretrain baseline.\nModels CR NQ HotpotQA Musique 2Wikiqa Average\nNormal\nMistral-7B 32x 54.25 43.11 9.85 45.84 38.26\nw/mse 32x 54.64+0.39 43.52+0.41 10.55+0.70 46.58+0.74 38.82+0.56\nMistral-7B 128x 52.98 41.32 10.22 46.23 37.69\nw/mse 128x 53.36+0.38 41.37+0.05 10.26+0.04 46.40+0.17 37.85+0.16\nTable21 Instruction-tuning performance with and without the MSE loss under different compression ratios (CR = 32,\n128) and normal retrieval settings.\n34", "meta": {"page": 34, "source": "2511.18659v2.pdf"}}
{"text": "Prompt for Complex Question Generation\nYou are given a document delimited by <doc> and </doc>. Your job is to generate a set\nof MULTI-HOP questions that, taken together, comprehensively cover the document‚Äôs key\ninformation.\n<doc>\n<INSERT DOCUMENT HERE>\n</doc>\nQuestion Requirements:\n1) Self-contained: Every question must be understandable without viewing the document.\n2) Multi-hop only: Each question must require at least TWO independent\npieces of evidence from DIFFERENT parts of the document (e.g., different\nparagraphs/sections/tables/items). If a question can be answered from a single sentence\nor data point, REJECT it.\n3) Non-overlapping: No two questions may target the same fact or the same combination of\nfacts. Each question must have a unique reasoning path and evidence combination.\n4) Coverage: Produce as many questions as needed to cover ALL key facts in the document.\nPrefer many small, precise multi-hop questions over a few large ones.\n5) Focus: Each question should target ONE multi-hop objective, typically integrating\n2‚Äì3 facts (bridging, comparison, aggregation, temporal/causal linking, entity‚Äìattribute\njoining, etc.). Do NOT bundle multiple unrelated sub-questions.\n6) Verifiability: The answer to each question must be derivable SOLELY from the document,\nwith no external knowledge or subjective judgment.\n7) Clarity: Avoid yes/no questions and vague wording. Use explicit constraints,\nquantities, and identifiers where relevant.\n8) No explanations: Do NOT include rationales, steps, or references‚ÄîONLY output the\nquestions as JSON.\n9) You can generate 2-hops, 3-hops, 4-hops, etc. questions.\nQUESTION TEMPLATES (use as patterns, adapt as needed)\n- Bridging: \"Which X satisfies BOTH condition A mentioned in [context A] AND condition B\nmentioned in [context B]?\"\n- Comparison: \"Considering [pivot], which of X or Y meets [criterion] when combining\ndetails from [source 1] and [source 2]?\"\n- Aggregation: \"When combining [quantity/info] from section A with [quantity/info] from\nsection B, which single entity matches [combined constraint]?\"\n- Temporal/Causal: \"Based on the timeline described in parts A and B, which event/entity\nfulfills [temporal/causal relation]?\"\nThere are some examples: {3 demonstrations}\nInput FORMAT:\nDocument:\n<INSERT DOCUMENT HERE>\nOUTPUT FORMAT\nReturn ONLY a JSON object with keys \"Question1\", \"Question2\", ..., \"QuestionN\".\nExample (structure only):\n{ \"Question1\": \"...\",\n\"Question2\": \"...\",\n\"Question3\": \"...\",\n\"QuestionN\": \"...\" }\nFigure10 Prompt used for complex question generation.\n35", "meta": {"page": 35, "source": "2511.18659v2.pdf"}}
{"text": "Prompt for Answer Generation\nYou are a factual answering assistant.\nYour task is to read the provided document and answer the given question **based only\non the information explicitly stated in the document**.\nPlease output the answer as short as possible.\nRequirements:\n- Your answer must be based solely on the content of the document.\n- Do not use prior knowledge or make assumptions beyond the document.\n- If the document does not contain the answer, respond with: \"The document does not\ncontain this information.\"\n- The answer should be concise, factual, and complete.\nInput Format:\nDocument:\n<INSERT DOCUMENT TEXT HERE>\nQuestion:\n<INSERT QUESTION HERE>\nOutput Format:\nAnswer: <YOUR ANSWER HERE>\nFigure11 Prompt used for factual answer generation.\n36", "meta": {"page": 36, "source": "2511.18659v2.pdf"}}
{"text": "Prompt for QA Validation\nYou are a fact-checking assistant.\nYour task is to verify whether the given answer to a question is **fully supported by\nthe provided document**.\nInstructions:\n- Read the document carefully.\n- Read the question and the provided answer.\n- Determine whether the answer is correct **based solely on the information in the\ndocument**.\n- The answer must be **complete**, **factually correct**, and **not contain any\ninformation that is not in the document**.\nIf the answer is fully correct and supported by the document, respond with:\n\"Correct\"\nIf the answer is partially correct, incomplete, or includes unsupported information,\nrespond with:\n\"Incorrect\"\nInput Format:\nDocument:\n<INSERT DOCUMENT HERE>\nQuestion:\n<INSERT QUESTION HERE>\nAnswer:\n<INSERT ANSWER HERE>\nOutput Format:\n{{\"Judgment\": \"Correct\" / \"Incorrect\"}}\nFigure12 Prompt used for QA validation.\n37", "meta": {"page": 37, "source": "2511.18659v2.pdf"}}
{"text": "Prompt for Supplementary Simple QA Generation\nYou are given a document and a set of existing question-answer pairs. Your task is to\ncarefully compare the information covered in the QA pairs against the document and\ngenerate additional questions that cover any key information not yet addressed.\nRequirements:\n- Only generate questions for key facts present in the document that are **not already\ncovered** in the existing QA pairs.\n- Do **not** repeat or rephrase the information in the existing question answer pairs.\n- Each question should cover **only one or two distinct key pieces of information**.\n- Each question must be self-contained, meaning it should be understood by the user\nwithout seeing the document.\n- All questions should require **simple factual recall only**, with no inference or\nreasoning.\nThere are some examples:\n{3 demonstrations}\nInput Format:\nDocument:\n<INSERT DOCUMENT HERE>\nExisting QA:\n<INSERT EXISTING QA HERE>\nOutput Format:\nReturn your generated new supplementary questions in the following JSON format:\n{\n\"Number of Supplementary Questions\": N,\n\"Question1\": \"...\",\n\"Question2\": \"...\",\n...,\n\"QuestionN\": \"...\"\n}\nIf all key information is already covered and no supplementary questions are needed,\noutput an empty JSON object:\n{\n\"Number of Supplementary Questions\": 0\n}\nFigure13 Prompt used for supplementary Simple QA generation.\n38", "meta": {"page": 38, "source": "2511.18659v2.pdf"}}
{"text": "Prompt for Supplementary Complex QA Generation\nYou are given a document and a set of existing question-answer pairs. Your task is to\ncarefully compare the information covered in the QA pairs against the document and generate\nadditional MULTI-HOP questions that cover any key information not yet addressed.\nRequirements:\n- Only generate questions for key facts present in the document that are **not already\ncovered** in the existing question answer pairs.\n- Do **not** repeat or rephrase information which can be found in the existing question answer\npairs.\nQuestion Requirements:\n1) Self-contained: Every question must be understandable without viewing the document.\n2) Multi-hop only: Each question must require at least TWO independent pieces of evidence\nfrom DIFFERENT parts of the document (e.g., different paragraphs/sections/tables/items). If a\nquestion can be answered from a single sentence or data point, REJECT it.\n3) Non-overlapping: No two questions may target the same fact or the same combination of\nfacts. Each question must have a unique reasoning path and evidence combination.\n4) Coverage: Produce as many questions as needed to cover ALL key facts in the document.\nPrefer many small, precise multi-hop questions over a few large ones.\n5) Focus: Each question should target ONE multi-hop objective, typically integrating 2‚Äì3\nfacts (bridging, comparison, aggregation, temporal/causal linking, entity‚Äìattribute joining,\netc.). Do NOT bundle multiple unrelated sub-questions.\n6) Verifiability: The answer to each question must be derivable SOLELY from the document,\nwith no external knowledge or subjective judgment.\n7) Clarity: Avoid yes/no questions and vague wording. Use explicit constraints, quantities,\nand identifiers where relevant.\n8) No explanations: Do NOT include rationales, steps, or references‚ÄîONLY output the questions\nas JSON.\n9) You can generate 2-hops, 3-hops, 4-hops, etc. questions.\nQUESTION TEMPLATES (use as patterns, adapt as needed)\n- Bridging: \"Which X satisfies BOTH condition A mentioned in [context A] AND condition B\nmentioned in [context B]?\"\n- Comparison: \"Considering [pivot], which of X or Y meets [criterion] when combining details\nfrom [source 1] and [source 2]?\"\n- Aggregation: \"When combining [quantity/info] from section A with [quantity/info] from\nsection B, which single entity matches [combined constraint]?\"\n- Temporal/Causal: \"Based on the timeline described in parts A and B, which event/entity\nfulfills [temporal/causal relation]?\"\nInput Format:\nDocument: <INSERT DOCUMENT HERE> Existing QA: <INSERT EXISTING QA HERE>\nOutput Format:\nNote, do not repeat or paraphrase existing questions. Instead, generate new multi-hop\nquestions for the missing information, and put the new questions in JSON format:\n{\n\"Number of Supplementary Questions\": N,\n\"Question1\": \"...\",\n...,\n\"QuestionN\": \"...\"\n}\nIf all key information is already covered and no supplementary questions are needed, output\nan empty JSON object:\n{\n\"Number of Supplementary Questions\": 0\n}\nFigure14 Prompt used for supplementary complex QA generation.\n39", "meta": {"page": 39, "source": "2511.18659v2.pdf"}}
{"text": "Prompt for Document Paraphrasing\nYou are given a document. Your task is to paraphrase the document in a way that:\n(1) **Restructure extensively** ‚Äî **Try your best to break down the structure of\nthe original document**, do not keep the same paragraphing, ordering, or sentence\nflow. Reorganize ideas, shuffle the order, merge or split sentences, and restructure\narguments.\n(2) **Preserve meaning with absolute accuracy** ‚Äî ensure that ALL key information\nand semantics of the original document are retained. Do not omit any factual details,\nnumbers, dates, or specific information.\n(3) **Avoid direct copying** ‚Äî no sentence should remain identical; re-express ideas in\na fresh way using synonyms and varied sentence structures.\n(4) **CRITICAL: Add no new information** ‚Äî the paraphrased document cannot introduce:\n- New facts, interpretations, or context not explicitly stated\n- Organizational names or affiliations not mentioned in the original\n- Explanatory details or background information\n- Your own analysis or conclusions about the content\n(5) **Maintain the original‚Äôs voice and perspective** ‚Äî if the original uses commands\n(\"followers shall...\"), don‚Äôt change it to descriptions (\"the organization promotes...\").\nPreserve the document‚Äôs intended tone and format.\n(6) **Verify factual relationships** ‚Äî when paraphrasing complex information involving\nmultiple entities, dates, or cause-and-effect relationships, double-check that you\nmaintain the correct connections and chronology.\n(7) **Use varied vocabulary** ‚Äî employ synonyms and alternative expressions while\nmaintaining precision.\n(8) **Preserve completeness** ‚Äî if the original mentions specific numbers, dates, names,\nor measurements, include them in your paraphrase (even if reworded).\n(9) **Maintain coherence** ‚Äî the paraphrased version should read as natural and fluent\nwriting.\n**WARNING: Do not assume context.** If a document mentions \"the Samaj\" without\nidentifying what organization this refers to, do not assume it‚Äôs \"Brahma Samaj\" or any\nother specific group. Work only with what is explicitly stated.\nProduce a paraphrased version that keeps the meaning and all factual details but has\nsignificantly altered structure and wording.\nHere are some examples of paraphrased documents.\n{10 demonstrations}\nInput:\n<document>\nFigure15 Prompt used for document paraphrasing.\n40", "meta": {"page": 40, "source": "2511.18659v2.pdf"}}
{"text": "Prompt for Paraphrase Validation\nYou are given two texts:\nOriginal Document ‚Äì the source text containing the key information.\nParaphrased Document ‚Äì a rewritten version of the original.\nYour task is to check whether the paraphrased document fully preserves all the key\ninformation from the original document, without adding any new information.\nGuidelines:\n(1) ‚Äò‚ÄòKey information‚Äô‚Äô means the essential facts, arguments, data, and main ideas of\nthe original.\n(2) The paraphrased document must:\n(2.1) Contain all the key information from the original.\n(2.2) Not omit any important point.\n(2.3) Not introduce information that is absent in the original.\n(2.4) Preserve meanings without distortion or contradiction.\n(2.5) Differences in style, sentence structure, or wording are acceptable as long as the\nmeaning is preserved.\n(3) Output format:\n(3.1) Answer ‚Äò‚ÄòYes‚Äô‚Äô if the paraphrased document completely retains all key information\nand introduces no new information.\n(3.2) Answer ‚Äò‚ÄòNo‚Äô‚Äô if any key information is lost, altered, or if extra/unwarranted\ninformation appears.\nDon‚Äôt output any explanation.\nInput:\nOriginal Document: [insert original doc]\nParaphrased Document: [insert paraphrased doc]\nOutput:\nYes / No\nFigure16 Prompt used for paraphrase validation.\nPrompt for Document-based QA\nYou are a helpful assistant. Your task is to extract relevant information from provided\ndocuments and to answer to questions as briefly as possible.\nBackground: {docs}\nQuestion:{question}\nFigure17 Prompt used for document-based question answering.\n41", "meta": {"page": 41, "source": "2511.18659v2.pdf"}}
