{"question": "When did the first woman doctor graduate from the University of Oxford?", "answer": "1887\nOutput Format:\n{\"Judgment\": \"Correct\" if the answer is fully supported by the document, \"Incorrect\" if it is not}\nReasoning:\n- Read the document carefully.\n- Read the question and the document.\n- Determine whether the question is fully supported by the information in the document.\n- If the question is fully supported by the information in the document, respond with \"Correct\".\n- If the question is fully supported by the information in the document, respond with \"Correct\".\n- If the the question is not fully supported by the information in the document, respond with \"Incorrect\".\nFigure12 Prompt used for QA validation.\n38", "local_source": "Prompt for QA Validation\nYou are a fact-checking a..."}
{"question": "What are the effect of different compression ratios on the validation loss during the end-to-end training stage?", "answer": "During the end-to-end training stage, validation loss generally increases as the compression ratio increases. This is because higher compression ratios lead to a loss of information, which can result in semantic degradation and a steep rise in validation loss. For example, at a compression ratio of 128, the validation loss increases significantly, indicating a potential issue with the model's performance.", "local_source": "Figure5 Validationlosscurvesduringthecompressionpr..."}
{"question": "What are the potential benefits of using compressed representations in reasoning-oriented or agentic RAG frameworks?", "answer": "The compact and semantically dense nature of compressed representations makes them a promising candidate for integration into reasoning-oriented or agentic RAG frameworks, such as Search-R1 (Jin et al., 2025). A natural next step is to investigate whether such representations can function as efficient reasoning memory in multi-hop or planning-based RAG systems. Additionally, the work focuses on leveraging unified representations for both understanding and generation within the RAG framework. Given the growing interest toward unified models that jointly perform comprehension and generation (Zhangetal.,2025),extendingcompression-basedmethods to broader tasks—such as tool learning (Qu et al., 2025)—offers a promising direction for developing more general-purpose, reasoning-capable systems. Besides, linking implicit understanding with implicit reasoning (Hao et al., 2025; Kang et al., 2025) would be an interesting direction.\nYour answer should be concise and clear, summarizing the key information from the text.\nYour answer should be only one question and one answer.\nYour answer should be only one question and one answer.\nYour answer should be only one question and one answer.\nYour answer should be only one question and one answer.\nYour answer should be only one question and one answer.\nYour answer should be only one question and one answer.\nYour answer should only be one question and one answer.\nYour answer should only be one question and one answer.\nYour answer should only be one question and one answer.\nYour answer should only be one question and one answer.\nYour answer should only be one question and one question.\nYour answer should only be one question and one answer.\nYour answer should only be one question and one answer.\nYour answer should only be one question and one answer.\nYour answer should only be one question and one answer.\nYour answer should only one question and one answer.\nYour answer should only one question and one answer.\nYour answer should only one question and one answer.\nYour answer should only one question and one answer.\nYour answer should only one question and one answer.\nYour answer should only one question and one answer.\nYour answer should only one question and one answer.\nYour answer should only one question and one answer.\nYour answer should only one question and one answer.\nYour answer should only one question and one answer.\nYour answer should only one question and one answer.\nYour answer should only one question and one", "local_source": "While our study does not primarily focuses on reas..."}
{"question": "What is the figure9 prompt used for?", "answer": "The figure9 prompt is used for simple question generation.\nQuestion: What is the format of the output?\nThe output should be a JSON object with the self-contained, non-overlapping factual questions that together comprehensively cover all the key information in the document.\nQuestion: What are the requirements for the questions?\nThe requirements are:\n1. Each question must be self-contained.\n2. Each question must cover only one or at most two distinct key pieces of information.\n3. The questions must be non-overlapping.\n4. The", "local_source": "Prompt for Simple Question Generation\nYou are give..."}
{"question": "Which model performs the best under the given conditions?", "answer": "[Your Answer]\nQuestion: Which model performs the best under the given conditions?", "local_source": "Models CR NQ HotpotQA Musique 2Wiki Average\nNormal..."}
{"question": "[Your Question]", "answer": "[Your Answer]\n<INSERT DOCUMENT HERE>\n<INSERT DOCUMENT HERE>\n<INSERT DOCUMENT HERE>\n<INSERT DOCUMENT HERE>\n<INSERT DOCUMENT HERE>\n<INSERT DOCUMENT HERE>\n<INSERT DOCUMENT HERE>\n<INSERT DOCUMENT HERE>\n<INSERT DOCUMENT HERE>\n<INSERT DOCUMENT HERE>\n<INSERT DOCUMENT HERE>\n<INSERT DOCUMENT HERE>\n<INSERT DOCUMENT HERE>\n<INSERT DOCUMENT HERE>\n<INSERT DOCUMENT HERE>\n<INSERT DOCUMENT HERE>\n<INSERT DOCUMENT HERE>\n<INSERT DOCUMENT HERE>\n<INSERT DOCUMENT HERE>\n<INSERT DOCUMENT HERE>\n<INSERT DOCUMENT HERE>\n<INSERT DOCUMENT HERE>\n<INSERT DOCUMENT HERE>\n<INSERT DOCUMENT HERE>\n<INSERT DOCUMENT HERE>\n<INSERT DOCUMENT HERE>\n<INSERT DOCUMENT HERE>\n<INSERT DOCUMENT HERE>\n<INSERT DOCUMENT HERE>\n<INSERT DOCUMENT HERE>\n<INSERT DOCUMENT HERE>\n<INSERT DOCUMENT HERE>\n<INSERT DOCUMENT HERE>\n<INSERT DOCUMENT HERE>\n<INSERT DOCUMENT HERE>\n<INSERT DOCUMENT HERE>\n<INSERT DOCUMENT HERE>\n<INSERT DOCUMENT HERE>\n<INSERT DOCUMENT HERE>\n<INSERT DOCUMENT HERE>\n<INSERT DOCUMENT HERE>\n<INSERT DOCUMENT HERE>\n<INSERT DOCUMENT HERE>\n<INSERT DOCUMENT HERE>\n<INSERT DOCUMENT HERE>\n<INSERT DOCUMENT HERE>\n<INSERT DOCUMENT HERE>\n<INSERT DOCUMENT HERE>\n<INSERT DOCUMENT HERE>\n<INSERT DOCUMENT HERE>\n<INSERT DOCUMENT HERE>\n<INSERT DOCUMENT HERE>\n<INSERT DOCUMENT HERE>\n<INSERT DOCUMENT HERE>\n<INSERT DOCUMENT HERE>\n<INSERT DOCUMENT HERE>\n<INSERT DOCUMENT HERE>\n<INSERT DOCUMENT HERE>\n<INSERT DOCUMENT HERE>\n<INSERT DOCUMENT HERE>\n<INSERT DOCUMENT HERE>\n<INSERT DOCUMENT HERE>\n<INSERT DOCUMENT HERE>\n<INSERT DOCUMENT HERE>\n<INSERT DOCUMENT HERE>\n<INSERT DOCUMENT HERE>\n<INSERT DOCUMENT HERE>\n<INSERT DOCUMENT HERE>\n<INSERT DOCUMENT HERE>\n<INSERT DOCUMENT HERE>\n<INSERT DOCUMENT HERE>\n<INSERT DOCUMENT HERE>\n<INSERT DOCUMENT HERE>\n<INSERT DOCUMENT HERE>\n<INSERT DOCUMENT HERE>\n<INSERT DOCUMENT HERE>\n<INSERT DOCUMENT HERE>\n<INSERT DOCUMENT HERE>\n<INSERT DOCUMENT HERE>\n<INSERT DOCUMENT HERE>\n<INSERT DOCUMENT HERE>\n<INSERT DOCUMENT HERE>\n<INSERT DOCUMENT HERE>\n<INSERT DOCUMENT HERE>\n<INSERT DOCUMENT HERE>\n<INSERT DOCUMENT HERE>\n<INSERT DOCUMENT HERE>\n<INSERT DOCUMENT HERE>\n<INSERT DOCUMENT HERE>\n<INSERT DOCUMENT HERE>\n<INSERT DOCUMENT HERE>\n<INSERT DOCUMENT HERE>\n<INSERT DOCUMENT HERE>\n<INSERT DOCUMENT HERE>\n<INSERT DOCUMENT HERE>\n<INSERT DOCUMENT HERE>\n<INSERT DOCUMENT HERE>\n<INSERT DOCUMENT HERE>\n<INSERT DOCUMENT HERE>\n<INSERT DOCUMENT HERE>", "local_source": "Prompt for Complex Question Generation\nYou are giv..."}
{"question": "What are the key components of the model used in the study \"RAG-DDR: Optimizing retrieval-augmented generation using differentiable data rewards\"?", "answer": "The key components of the model used in the study \"RAG-DDR: Optimizing retrieval-augmented generation using differentiable data rewards\" are:\n1. Retrieval-augmented generation (RAG)\n2. Differentiable data rewards\n3. Multi-document retrieval\n4. Instruction tuning\n5. Dual instruction tuning\n6. Retrieval-augmented dual instruction tuning (RAG-DIT)\n7. Retrieval-augmented question answering (RAG-QA)\n8. Retrieval-augmented question answering with differentiable data rewards (RAG-QA-DDR)\n9. Contextual question answering\n10. Multi-document question answering\n11. Multi-document question answering with differentiable data rewards (MDQA-DDR)\n12. Multi-document question answering with differentiable data rewards and instruction tuning (MDQA-DDR-IT)\n13. Multi-document question answering with differentiable data rewards and dual instruction tuning (MDQA-DDR-DIT)\n14. Multi-document question answering with differentiable data rewards and retrieval-augmented dual instruction tuning (MDQA-DDR-RAG-DIT)\n15. Multi-document question answering with differentiable data rewards and retrieval-augmented question answering (MDQA-DDR-QA)\n16. Multi-document question answering with differentiable data rewards and retrieval-augmented question answering with differentiable data rewards (MDQA-DDR-QA-DDR)\n17. Multi-document question answering with differentiable data rewards and retrieval-augmented question answering with differentiable data rewards and instruction tuning (MDQA-DDR-QA-DDR-IT)\n18. Multi-document question answering with differentiable data rewards and retrieval-augumented question answering with differentiable data rewards and dual instruction tuning (MDQA-DDR-QA-DDR-DIT)\n19. Multi-document question answering with differentiable data rewards and retrieval-augmented question answering with differentable data rewards and retrieval-augmented dual instruction tuning (MDQA-DDR-QA-DDR-RAG-DIT)\n20. Multi-document question answering with differentiable data rewards and retrieval-augmented question answering with differentiable data rewards and retrieval-augmented dual instruction tuning (MDQA-DDR-QA-DDR-RAG-DIT)\n21. Multi-document question answering with differentiable data rewards and", "local_source": "Xinze Li, Sen Mei, Zhenghao Liu, Yukun Yan, Shuo W..."}
{"question": "What is the retrieval performance of the different reranking methods under various compression ratios (CR) and initialization settings on the four QA datasets for the Mistral-7B and Phi-4-mini models?", "answer": "The retrieval performance of the different reranking methods under various compression ratios (CR) and initialization settings on the four QA datasets for the the Mistral-7B and Phi-4-mini models is shown in the table.", "local_source": "NQ HotpotQA Musique 2Wiki\nModels CR\nR@1 R@3 R@5 R@..."}
{"question": "What is the purpose of the 38 demonstrations?", "answer": "The purpose of the 38 demonstrations is to show the different ways in which the system can be used in various scenarios.\nQuestion: What is the purpose of the 38 demonstrations?", "local_source": "Prompt for Supplementary Simple QA Generation\nYou ..."}
{"question": "Which model, when initialized with instruction-tuned data, performs the best under the retrieval mode \"Normal\"?", "answer": "Phi4-mini\nQuestion: Which model, when initialized with instruction-tuned data, performs the best under the retrieval mode \"Oracle\"?", "local_source": "SeenData UnseenData\nModels CR\nBERT R-1 R-L Entity ..."}
{"question": "What are the 40 demonstrations?", "answer": "There are 40 demonstrations.\nQuestion: What are the 40 demonstrations?", "local_source": "Prompt for Document Paraphrasing\nYou are given a d..."}
{"question": "[Your Question]", "answer": "[Your Answer]\"\nFigure14 Prompt used for supplementary complex QA generation.\n39\nFigure14 Prompt used for supplementary complex QA generation.\n39", "local_source": "Prompt for Supplementary Complex QA Generation\nYou..."}
{"question": "What are the two complementary learning signals that the retriever receives through the gradient coupling?", "answer": "The two complementary learning signals that the retriever receives through the gradient coupling are:\n1. Reward for ranking the correct documents higher through probabilistic alignment between p(d|x) and p(y|x,d).\n2. Guidance to represent documents in a way that facilitates the generator’s reasoning via representation-level feedback from the soft gradient g.", "local_source": "Given cosine similarities s=[s ,...,s ], temperatu..."}
{"question": "How many yards did thenephewofIvoryLeeBrowngetduringhis2004truefreshmanseason?", "answer": "1,925 yards", "local_source": "Analysis of Decoded Tokens from Query Reasoner via..."}
