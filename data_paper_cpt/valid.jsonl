{"text": "A GradientsforNon-sharedvs.SharedRepresentationsinRAG\nStep1: Let s xd be the retrieval score for query x and document d, and let\np(d|x)= exp(s xd ) , p(y|x)= (cid:88) p(d|x)p(y|x,d), L=‚àílogp(y|x). (A.1)\n(cid:80)\nexp(s )\nd‚Ä≤‚ààC xd‚Ä≤ d‚ààC\nStep2:productruleinsidethesum.\n‚àÇ (cid:16)(cid:88) (cid:17) (cid:88) ‚àÇp(d‚Ä≤|x) (cid:88) ‚àÇp(y|x,d‚Ä≤)\np(d‚Ä≤|x)p(y|x,d‚Ä≤) = p(y|x,d‚Ä≤)+ p(d‚Ä≤|x) . (A.2)\n‚àÇs ‚àÇs ‚àÇs\nxd xd xd\nd‚Ä≤ d‚Ä≤ (cid:124) (cid:123)(cid:122) (cid:125) d‚Ä≤\nsoftmaxJacobian\nStep3:softmaxJacobian. For p(d‚Ä≤|x)= (cid:80) esx e d s ‚Ä≤ xj ,\nj\n‚àÇp(d‚Ä≤|x)\n=p(d‚Ä≤|x) (cid:0) 1[d‚Ä≤ =d]‚àíp(d|x) (cid:1) .\n‚àÇs\nxd\nTherefore\n(cid:88)‚àÇp(d‚Ä≤|x) p(y|x,d‚Ä≤)= (cid:88) p(d‚Ä≤|x) (cid:0) 1[d‚Ä≤ =d]‚àíp(d|x) (cid:1) p(y|x,d‚Ä≤) (A.3)\n‚àÇs\nxd\nd‚Ä≤ d‚Ä≤\n(cid:88)\n=p(d|x)p(y|x,d)‚àíp(d|x) p(d‚Ä≤|x)p(y|x,d‚Ä≤) (A.4)\nd‚Ä≤\n=p(d|x) (cid:0) p(y|x,d)‚àíp(y|x) (cid:1) . (A.5)\nPlugging equation A.5 into equation A.2 and then equation A.1 gives\nStep4:puttogether.\nÔ£Æ Ô£π\n‚àÇL =‚àí 1 Ô£Ø Ô£Ø Ô£Øp(d|x) (cid:0) p(y|x,d)‚àíp(y|x) (cid:1) + (cid:88) p(d‚Ä≤|x) ‚àÇp(y|x,d‚Ä≤) Ô£∫ Ô£∫ Ô£∫. (A.6)\n‚àÇs p(y|x)Ô£Ø ‚àÇs Ô£∫\nxd Ô£∞(cid:124) (cid:123)(cid:122) (cid:125) d‚Ä≤ xd Ô£ª\n(I)probabilitypath (cid:124) (cid:123)(cid:122) (cid:125)\n(II)representation/generationpath\nStep5:commonsimplification(assumption). If the generator‚Äôs conditional p(y|x,d‚Ä≤) depends on s xd only when\nd‚Ä≤ =d (e.g., each conditional uses its own selected document; non-shared case gives it zero), then the second\nsum reduces to a single term:\n(cid:88) ‚àÇp(y|x,d‚Ä≤) ‚àÇp(y|x,d)\np(d‚Ä≤|x) = p(d|x) .\n‚àÇs ‚àÇs\nxd xd\nd‚Ä≤\nUnder this widely-used assumption, equation A.6 becomes\n‚àÇL =‚àí 1 (cid:104) p(d|x) (cid:0) p(y|x,d)‚àíp(y|x) (cid:1) + p(d|x) ‚àÇp(y|x,d) (cid:105) . (A.7)\n‚àÇs p(y|x) ‚àÇs\nxd (cid:124) (cid:123)(cid:122) (cid:125) xd\n(I)probabilitypath (cid:124) (cid:123)(cid:122) (cid:125)\n(II)representation/generationpath\nRemark(moregeneralshared-conditioning). If the generator conditions on a mixture r = (cid:80) j œÄ j (s)z j (so every\np(y|x,d‚Ä≤) shares the same r), then ‚àÇp(y|x,d‚Ä≤) is the same for all d‚Ä≤ and (cid:80) p(d‚Ä≤|x)‚àÇp(y|x,d‚Ä≤) = ‚àÇp(y|x,r). Both\nforms are consistent; the boxed formu ‚àÇ la sxdcorresponds to the per-documen d t ‚Ä≤ conditiona ‚àÇ l s vxdiew. ‚àÇsxd\nTerm (II) is present if the generator‚Äôs conditional p(y|x,d) depends (directly or indirectly) on s .\nxd\n16", "meta": {"page": 16, "source": "2511.18659v2.pdf"}}
{"text": "Algorithm 1 Differentiable Top-k Selection with Straight-Through Estimator in CLaRA\n1: Input: Similarityscoress‚ààRB√óD,temperatureœÑ,numberofselectionsk\n2: Output: SelectiontensorZ‚ààRB√ók√óD andtop-k indices{rj}k\nj=1\n3: sÀú‚Üês/max(œÑ,10‚àí6)\n4: InitializeZ ,Z ‚Üê0B√ók√óD, taken‚Üê0B√óD\nhard soft\n5: forj=1tok do\n6: (1) Hard selection: rj ‚ÜêargmaxisÀú(:,i)onunmaskedcandidates\n7: Z\nhard\n[:,j,rj]‚Üê1\n8: (2) Soft selection: mask‚Üê1‚àíSG(taken)\n9: logits ‚ÜêsÀú+log(mask+Œµ)\nj\n10: pj ‚Üêsoftmax(logits\nj\n)\n11: Z\nsoft\n[:,j,:]‚Üêpj\n12: taken‚Üêmin(taken+Z [:,j,:],1)\nhard\n13: endfor\n14: (3) Straight-through estimator: Z‚ÜêZ +(Z ‚àíSG(Z ))\nhard soft soft\n15: return(Z,{rj}k\nj=1\n)\nthen the required Jacobians are\n(cid:18) (cid:19) (cid:18) (cid:19)\n‚àÇs 1 q ‚àÇs 1 z\nxd = z ‚àís ‚à•z ‚à• , xd = q‚àís d ‚à•q‚à• . (A.16)\n‚àÇq ‚à•q‚à•‚à•z ‚à• d xd ‚à•q‚à•2 d ‚àÇz ‚à•q‚à•‚à•z ‚à• xd ‚à•z ‚à•2\nd d d d\nHence\n‚àÇL = (cid:88) ‚àÇL ‚àÇs xd, ‚àÇL = ‚àÇL ‚àÇs xd. (A.17)\n‚àÇq ‚àÇs ‚àÇq ‚àÇz ‚àÇs ‚àÇz\nxd d xd d\nd‚ààC\nSimple QA Complex QA Paraphrase Doc\nNum. 2,000,000 2,000,000 1,966,291\nAvg.pairs 7.80 4.62 1.00\nAvg.inp 95.56 95.56 95.56\nAvg.out 158.18 253.90 108.67\nTable9 Pretraining data statistics for SCP. The table reports the total number of training examples (Num.), average\nnumber of generated QA pairs or documents (Avg.pairs), average input document length (Avg.inp) and average\ngenerated text length (Avg.out) for Simple QA, Complex QA, and Paraphrased Documents..\nDatasets TrainingDataSize EvaluationDataSize\nNatureQuestion 58,622 6,489\nHotpotQA 90,185 7,384\nMusiQue 168,745 2,417\n2WikiMultiHopQA 167,454 12,576\nTable10 Statistics of experimental datasets.\nB Detailedexperimentalsetup\nB.1 Datasets\nThe pretraining corpus consists of 2M documents and their corresponding 2M SimpleQA sets, 2M ComplexQA\nsets, and 2M paraphrased documents. Detailed statistics on data composition and distribution are provided\nin Table 9.\nDuring the instruction tuning stage of compression learning, we use question data from COCOM (Rau\net al., 2025) , which contains 453k questions. We employ the Mistral-7B model and retrieve the top-5 most\n18", "meta": {"page": 18, "source": "2511.18659v2.pdf"}}
{"text": "Compressor Pre-Training Stage Query: Query embedding Doc embeddings Answer:\nRaw Docs Compressed Docs Salient info Which Retrieve New\ncity is the York City\nCompressor Generator living\nplace of Big Stone Gapis a 2014\nthe city, big, Stone, A co m m e e r d ic y a f n i l d m r a d m ir a e c r t o e m d a n b t y i c\ndirector ciudad, Based, Adriana Trigiani.The story is\nre Q a u s e o r n y e r Retrieved Generator o ro f m th an e t ic r V lo o i v r m , g a K in n a i t t a i e c , , , c s R h t a o o r m , m , v i a n r g e , , s o e f t B i i n g t S h t e o n ac e t G ua a l p V c i i r r g c i a n i 1 a 9 7 t 0 ow s. n\nCompressed Docs comedy d an ir a e , c l t o o c r a , t f io il n m , , v i r l i l , a ge, Adriana Trigiani is an Italian\nQuery Co t n o t k i e n n u s ous Backward Forward Answer ‚Äò G B a ig p ‚Äô S ? tone r m e o s v i i d e ence, lives, A t d V e i i m r l l l e e e a c v g r i t e i s c o , i a o r N n n b e b w a w e s r e s i Y t t d e o - i r s r n , e k a l G l n i C r n d i e g t e y f a n i . u l w m t i h c o h r ,\nCLaRa End-to-end Training Stage CLaRa End-to-end Inference Stage\nFigure1 (a) During training, we first pretrain the compressor to encourage it to retain only essential information.\nNext,weperformofflinecompressionofthedocuments. Afterthat,weencodethequeryusingthequeryreasoner,\nretrieve the compressed document representations for generation, and use only the final next-token prediction\nloss to jointly up Q d ue a ry t e both the query reasoner and the generator. (b) An example from the inference stage: the\ntokens representreaksoenyerclue wo E r m dW be so d r d dr i n e g l ated to the question. When we decode the continuous query embedding, we find\nthat it contains infùêêorm L a oo t k i u o p n noMtatprixresent in the original query, indicating that it has learned some of the intermediate\nreasoning keywords.\nRelated documents: Adriana Trigiani is an Italian American best-selling author, television\nwriter, and film director based in Greenwich Village, New York City.\nBig Stone Gap is a 2014 American drama romantic comedy\nmemory-tokfeilmn wrriettpenr aensd edinretcatetdi boyn Asdritanha aTrtigisanei.r Tvhee stboroy tish setp inu trhpe aocstueasl V.irgAinia ctoewnn tofr al motivation behind this design is that\nBig Stone Gap circa 1970s.\nsupervisedretrievaltrainingtypicallyreliesonrelevance-labeleddata,whichisscarceandoftendomain-specific.\nTo overcome this limitation, we propagate the next-token prediction (NTP) loss from the generator to the\nretriever, providing a weakly supervised signal that naturally adapts retrieval to downstream generation\nobjectives. This mechanism allows the retriever to learn which documents truly enhance answer generation\nrather than relying on surface-level similarity. Moreover, continuous representations and joint optimization\nare inherently complementary: continuous encodings make the retrieval process differentiable, while joint\ntraining aligns both modules within a shared semantic space optimized for reasoning.\nThis unified design resolves both challenges simultaneously. Optimization-wise, continuous representations\nenable differentiable top-k selection via Straight-Through estimation, allowing generator gradients to update\nthe retriever directly through gradient descent rather than inefficient RL sampling. Efficiency-wise, shared\nencodings remove redundant computation and drastically reduce context length, enabling fully end-to-end\noptimization and inference within the same representation space.\nTo realize this vision, we present CLaRa (Continuous Latent Reasoning), a joint retrieval‚Äìgeneration\nframework built on shared compressed representations. In Stage I, we propose SCP (Salient Compressor\nPretraining), which enhances semantic fidelity by constructing QA pairs that emphasize salient document\ncontent beyond surface reconstruction. In Stage II, CLaRa performs end-to-end joint training of the\nquery encoder and answer generator under a unified next-token prediction loss, with differentiable top-k\nselectionviaStraight-Throughestimation. Theoretically, weshowthisunifiedobjectiveyieldsvalidgradients\nfor retriever learning without explicit labels.\nWe evaluate CLaRa on four single-hop and multi-hop QA benchmarks with Mistral-7B and Phi-4B. Results\nshow that SCP produces semantically rich compressed representations, and CLaRa achieves state-of-the-art\nretrieval and generation performance‚Äîoutperforming both supervised and unsupervised baselines, and even\nsurpassing full-text fine-tuned models on several tasks.\n2 SCP:SalientCompressorPretraining\nPrevious methods (Louis et al., 2025a; Cheng et al., 2025) typically use token-level reconstruction loss to learn\ndocrepresentation. However, thelearnedrepresentationmaywastelimitedcapacity/budgetontoken-by-token\nreconstruction which might be trivial. Also, the raw representation learned in such a way might not ‚Äúdigest‚Äù\nthedocumentexhaustively. Toenablethemodeltofocusonabstractinganddigestingsemanticallyinformative\nrepresentations, we first synthesize pre-training data that highlights salient information. Based on this data,\nwe train a compression framework, where a compressor learns to retain merely essential semantics (Figure 2).\n2", "meta": {"page": 2, "source": "2511.18659v2.pdf"}}
{"text": "For compressor evaluation, we benchmark against both classical and recent methods, including\nBaselines\nAutoCompressor (Chevalier et al., 2023), XRAG(Cheng et al., 2025), COCOM (Rau et al., 2025), PCC\n(Dai et al., 2025), LLMLingual-2 (Pan et al., 2024), and PISCO (Louis et al., 2025a). For reranking,\nwe compare with BM25, BGE-Reranker (Chen et al., 2023), RankZephyr-7B (Pradeep et al., 2023),\nSetwise (Zhuang et al., 2024), and Rank-R1 (Zhuang et al., 2025). End-to-end QA results are evaluated\nagainst representative RAG systems, including prompt-based (GenGround (Shi et al., 2024), In-Context\nRAG), retrieval-optimized (ReComp (Xu et al., 2024), DPA-RAG (Dong et al., 2025)), fine-tuned LLMs\n(Self-RAG (Asai et al., 2024), Retrobust (Yoran et al., 2024), ChatQA (Liu et al., 2025)), and jointly\noptimized models (DDR-RAG (Li et al., 2024a), DRO (Shi et al., 2025)). Unlike all baselines operating\non raw text, our method is the first to jointly optimize reranking and generation directly over compressed\nrepresentations. Full experimental settings are provided in Appendix B. Below, we summarize the key\nfindings, while the complete set of additional experiments can be found in the Appendix, including pretraining\ndata analysis (App.C & D), training process analysis (App.E), fidelity and grounding evaluations (App.F), as\nwell as further module analyses (App.G).\n4.2 EvaluationofCompressionEffectiveness\nWe evaluate our document compressor under two settings: Normal and Oracle. In the Normal setting,\nthe model retrieves the top-5 documents from Wikipedia-2021 for each query. In the Oracle setting, the\nannotated positive document is included among the top-5 to isolate compression quality from retrieval noise.\nTable 2 summarizes results across compression ratios. For full results, please refer to table 6 Our method con-\nsistently outperforms all baselines. Compared to the best soft compression model PISCO, our model achieves\naverage gains of 1.13% (Normal) and 5.35% (Oracle); over the hard compression baseline LLMLingual-2,\nimprovements reach 5.37% and 17.31%, highlighting stronger semantic preservation.\nSurprisingly, our model exceeds the text-based w/ BGE retrieval baseline using uncompressed documents,\nwith average gains of 2.36% on Mistral-7B and 6.36% on Phi-4-min. This implies that well-trained\nsoft compression can retain essential reasoning information while substantially reducing input length. This\nmay be because the compressed representations filter out irrelevant content and focus the generator on the\nreasoning-relevant context, leading to better generalization than raw text inputs. While performance declines\nat extreme compression (beyond 32√ó in Oracle), the drop remains moderate under Normal conditions due to\nweaker document relevance.\n4.3 JointTrainingResults\nForend-to-endlearning,weevaluateourmodelunderbothNormalandOraclesettings. IntheNormalsetup,\neach query retrieves the top-20 documents from Wikipedia-2021; the Oracle setup adds annotated positives\nto the 20-document pool to isolate generation quality from retrieval noise. We compare two initialization\nstrategies for joint reranking‚Äìgeneration training: (i) from the compression pretraining checkpoint, and (ii)\nfrom the instruction-tuned compressor. Results are shown in Table 3, with full results in Table 7 in Appendix.\nUnder the Normal setting, performance remains stable across compression ratios, peaking at 16‚Äì32√ó. As\n4√ó might be harder to optim w/ NTP, CLaRa-Mistral-7B with 16x surpasses the text-based DRO-\nMistral-7B, improving F1 from 51.01‚Üí51.41 on NQ and 43.65‚Üí47.18 on 2Wiki. In the Oracle setting,\nperformance rises notably‚ÄîF1 exceeds 75% on both NQ and HotpotQA‚Äîshowing that joint optimization\neffectively exploits accurate retrieval.\nInstruction-tunedinitializationoutperformspretraining-basedinitializationunderNormalconditions,especially\non NQ and HotpotQA, indicating stronger alignment between compression and answering. However, the gap\nnarrows in the Oracle setting, suggesting initialization matters less when retrieval is reliable. Overall, CLaRa\ndemonstrates robust and scalable performance across retrieval qualities and compression ratios.\n4.4 Retrievalperformance\nWe evaluate our method on the document reranking task to assess retrieval effectiveness under the Oracle\nsetting, where positive documents are guaranteed in the candidate set, allowing accurate computation of\n8", "meta": {"page": 8, "source": "2511.18659v2.pdf"}}
{"text": "Prompt for Paraphrase Validation\nYou are given two texts:\nOriginal Document ‚Äì the source text containing the key information.\nParaphrased Document ‚Äì a rewritten version of the original.\nYour task is to check whether the paraphrased document fully preserves all the key\ninformation from the original document, without adding any new information.\nGuidelines:\n(1) ‚Äò‚ÄòKey information‚Äô‚Äô means the essential facts, arguments, data, and main ideas of\nthe original.\n(2) The paraphrased document must:\n(2.1) Contain all the key information from the original.\n(2.2) Not omit any important point.\n(2.3) Not introduce information that is absent in the original.\n(2.4) Preserve meanings without distortion or contradiction.\n(2.5) Differences in style, sentence structure, or wording are acceptable as long as the\nmeaning is preserved.\n(3) Output format:\n(3.1) Answer ‚Äò‚ÄòYes‚Äô‚Äô if the paraphrased document completely retains all key information\nand introduces no new information.\n(3.2) Answer ‚Äò‚ÄòNo‚Äô‚Äô if any key information is lost, altered, or if extra/unwarranted\ninformation appears.\nDon‚Äôt output any explanation.\nInput:\nOriginal Document: [insert original doc]\nParaphrased Document: [insert paraphrased doc]\nOutput:\nYes / No\nFigure16 Prompt used for paraphrase validation.\nPrompt for Document-based QA\nYou are a helpful assistant. Your task is to extract relevant information from provided\ndocuments and to answer to questions as briefly as possible.\nBackground: {docs}\nQuestion:{question}\nFigure17 Prompt used for document-based question answering.\n41", "meta": {"page": 41, "source": "2511.18659v2.pdf"}}
